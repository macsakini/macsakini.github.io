<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Bert Analysis   Mac Code</title>
<meta name="description" content="!pip install -q tf-models-official !pip install -q tensorflow-text !pip install -q tf-models-official==2.3.0Sa">


  <meta name="author" content="Mac Maxwell">
  
  <meta property="article:author" content="Mac Maxwell">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Mac Code">
<meta property="og:title" content="Bert Analysis">
<meta property="og:url" content="http://localhost:4000/BERT-Analysis/">


  <meta property="og:description" content="!pip install -q tf-models-official !pip install -q tensorflow-text !pip install -q tf-models-official==2.3.0Sa">







  <meta property="article:published_time" content="2022-05-03T00:00:00+03:00">





  

  


<link rel="canonical" href="http://localhost:4000/BERT-Analysis/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Mac Maxwell",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Mac Code Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="https://media.istockphoto.com/photos/abstract-3d-render-of-triangle-picture-id1289515110?b=1&k=20&m=1289515110&s=170667a&w=0&h=NM9B-P3evH-Go1mZ1GtWFQZ4Hqtipuu90EO5ceRBeho=" alt="Mac Code"></a>
        
        <a class="site-title" href="/">
          Mac Code
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/_pages/home/">Home</a>
            </li><li class="masthead__menu-item">
              <a href="/">Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/_pages/cv/">Resume</a>
            </li><li class="masthead__menu-item">
              <a href="/_pages/portfolio/">Portfolio</a>
            </li><li class="masthead__menu-item">
              <a href="/_pages/publications/">Publications</a>
            </li><li class="masthead__menu-item">
              <a href="/_pages/contact/">Contact Me</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      


  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/" itemprop="item"><span itemprop="name">Home</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        <li class="current">Bert Analysis</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="https://avatars.githubusercontent.com/u/47692036?s=400&u=102797acf8ca76fa31a3bcf38afd98e238168b37&v=4" alt="Mac Maxwell" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Mac Maxwell</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>CS + Math @ UoN, SD @ Rarelane. Aspiring member of the LMS.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Nairobi, Kenya.</span>
        </li>
      

      
        
          
            <li><a href="mailto:macsakini@hotmail.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">macsakini@hotmail.com</span></a></li>
          
        
          
            <li><a href="https://www.rarelane.co.uk" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">https://www.rarelane.co.uk</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/macsakini/" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">Linkedin</span></a></li>
          
        
          
        
          
            <li><a href="https://github.com/macsakini" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">Mac Maxwell</span></a></li>
          
        
          
            <li><a href="https://www.researchgate.net/profile/Mac-Maxwell" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i><span class="label">Researchgate</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Bert Analysis">
    <meta itemprop="description" content="!pip install -q tf-models-official!pip install -q tensorflow-text!pip install -q tf-models-official==2.3.0Sa">
    <meta itemprop="datePublished" content="2022-05-03T00:00:00+03:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/BERT-Analysis/" class="u-url" itemprop="url">Bert Analysis
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          15 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">tf</span><span class="o">-</span><span class="n">models</span><span class="o">-</span><span class="n">official</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">tensorflow</span><span class="o">-</span><span class="n">text</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">tf</span><span class="o">-</span><span class="n">models</span><span class="o">-</span><span class="n">official</span><span class="o">==</span><span class="mf">2.3</span><span class="p">.</span><span class="mi">0</span><span class="n">Sa</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[K     |████████████████████████████████| 1.1MB 6.9MB/s 
[K     |████████████████████████████████| 51kB 6.8MB/s 
[K     |████████████████████████████████| 37.6MB 124kB/s 
[K     |████████████████████████████████| 174kB 50.3MB/s 
[K     |████████████████████████████████| 1.2MB 46.8MB/s 
[K     |████████████████████████████████| 276kB 41.9MB/s 
[K     |████████████████████████████████| 102kB 12.0MB/s 
[K     |████████████████████████████████| 358kB 44.5MB/s 
[?25h  Building wheel for seqeval (setup.py) ... [?25l[?25hdone
  Building wheel for pyyaml (setup.py) ... [?25l[?25hdone
  Building wheel for py-cpuinfo (setup.py) ... [?25l[?25hdone
[K     |████████████████████████████████| 3.4MB 5.0MB/s 
[K     |████████████████████████████████| 849kB 5.4MB/s 
[?25h
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">requests</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">post</span><span class="p">(</span><span class="s">"http://3386c69248d9.ngrok.io/"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s">'foo'</span><span class="p">:</span> <span class="s">'The Role of Saying No is sometimes seen as a luxury that only those in power can afford . But saying no is not merely a privilege reserved for the successful among us . It is also a strategy that can help you become successful . Steve Jobs famously said, “People think focus means saying yes to the thing you’ve got to focus on. But that’s not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully’re not always saying yes,” Steve Jobs said. “If you don’t guard your time, people will steal it from you," says Pedro Sorrentino. ‘If you are not guarding your time.’ says Sorrentinos.‘ If you want to say no to distractions, it means you need to say yes, it is the only productivity hack,’ he says. You may have to try many things to discover what works'</span><span class="p">})</span>
<span class="c1"># And done.
</span><span class="k">print</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">text</span><span class="p">)</span> <span class="c1"># displays the result body.
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> The Role of Saying No is sometimes seen as a luxury that only those in power can afford . But saying no is not merely a privilege reserved for the successful among us . It is also a strategy that can help you become successful
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Import dependencies
#Import necessary dependancies
</span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="kn">import</span> <span class="nn">tensorflow_hub</span> <span class="k">as</span> <span class="n">hub</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span> <span class="k">as</span> <span class="n">tfds</span>
<span class="n">tfds</span><span class="p">.</span><span class="n">disable_progress_bar</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">official.modeling</span> <span class="kn">import</span> <span class="n">tf_utils</span>
<span class="kn">from</span> <span class="nn">official</span> <span class="kn">import</span> <span class="n">nlp</span>
<span class="kn">from</span> <span class="nn">official.nlp</span> <span class="kn">import</span> <span class="n">bert</span>

<span class="c1"># Load the required submodules
</span><span class="kn">import</span> <span class="nn">official.nlp.optimization</span>
<span class="kn">import</span> <span class="nn">official.nlp.bert.bert_models</span>
<span class="kn">import</span> <span class="nn">official.nlp.bert.configs</span>
<span class="kn">import</span> <span class="nn">official.nlp.bert.run_classifier</span>
<span class="kn">import</span> <span class="nn">official.nlp.bert.tokenization</span>
<span class="kn">import</span> <span class="nn">official.nlp.data.classifier_data_lib</span>
<span class="kn">import</span> <span class="nn">official.nlp.modeling.losses</span>
<span class="kn">import</span> <span class="nn">official.nlp.modeling.models</span>
<span class="kn">import</span> <span class="nn">official.nlp.modeling.networks</span>

<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">files</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">numpy</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gs_folder_bert</span> <span class="o">=</span> <span class="s">"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12"</span>
<span class="n">tf</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="n">gfile</span><span class="p">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">gs_folder_bert</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['bert_config.json',
 'bert_model.ckpt.data-00000-of-00001',
 'bert_model.ckpt.index',
 'vocab.txt']
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hub_url_bert = "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2"
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uploaded = files.upload()

dataset = pd.read_csv(io.BytesIO(uploaded['profitaa.csv']))

df2 = dataset.sample(frac=0.8, random_state=0)
df2_test = dataset.drop(df2.index)

</code></pre></div></div>

<p><input type="file" id="files-020ab0fe-2813-4061-becf-21020407b709" name="files[]" multiple="" disabled="" style="border:none" /></p>
<output id="result-020ab0fe-2813-4061-becf-21020407b709">
 Upload widget is only available when the cell has been executed in the
 current browser session. Please rerun this cell to enable.
 </output>
<script src="/nbextensions/google.colab/files.js"></script>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Saving summary.csv to summary.csv
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>df2.shape

df2["Relevancy_Score"].isnull().values.any()

df3 = df2.dropna()

df3.describe().transpose()

</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>unique</th>
      <th>top</th>
      <th>freq</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>headline</th>
      <td>1487</td>
      <td>1487</td>
      <td>\nCreate a comfortable home for both hamsters....</td>
      <td>1</td>
    </tr>
    <tr>
      <th>title</th>
      <td>1487</td>
      <td>1487</td>
      <td>How to Tell a Middle School Boy You Like Him</td>
      <td>1</td>
    </tr>
    <tr>
      <th>text</th>
      <td>1487</td>
      <td>1486</td>
      <td>,,</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df3</span><span class="p">[</span><span class="s">"Relevancy_Score"</span><span class="p">].</span><span class="n">isnull</span><span class="p">().</span><span class="n">values</span><span class="p">.</span><span class="nb">any</span><span class="p">()</span>

<span class="n">df3_test</span> <span class="o">=</span> <span class="n">df2_test</span><span class="p">.</span><span class="n">dropna</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Set up tokenizer to generate Tensorflow dataset
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">bert</span><span class="p">.</span><span class="n">tokenization</span><span class="p">.</span><span class="n">FullTokenizer</span><span class="p">(</span>
    <span class="n">vocab_file</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">gs_folder_bert</span><span class="p">,</span> <span class="s">"vocab.txt"</span><span class="p">),</span>
     <span class="n">do_lower_case</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="n">s</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Vocab size:"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">vocab</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Vocab size: 30522
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s">"Hello TensorFlow!"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="n">ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['hello', 'tensor', '##flow', '!']
[7592, 23435, 12314, 999]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">([</span><span class="s">'[CLS]'</span><span class="p">,</span> <span class="s">'[SEP]'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[101, 102]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode_sentencer</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
   <span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
   <span class="n">tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">'[SEP]'</span><span class="p">)</span>
   <span class="k">return</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="n">sentence1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ragged</span><span class="p">.</span><span class="n">constant</span><span class="p">([</span>
    <span class="n">encode_sentencer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">df3</span><span class="p">[</span><span class="s">"Sentence_1"</span><span class="p">]])</span>
<span class="n">sentence2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ragged</span><span class="p">.</span><span class="n">constant</span><span class="p">([</span>
    <span class="n">encode_sentencer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">df3</span><span class="p">[</span><span class="s">"Sentence_2"</span><span class="p">]])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"Sentence1 shape:"</span><span class="p">,</span> <span class="n">sentence1</span><span class="p">.</span><span class="n">shape</span><span class="p">.</span><span class="n">as_list</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Sentence2 shape:"</span><span class="p">,</span> <span class="n">sentence2</span><span class="p">.</span><span class="n">shape</span><span class="p">.</span><span class="n">as_list</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">sentence1</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sentence1 shape: [1487, None]
Sentence2 shape: [1487, None]
tf.Tensor(
[ 4638  2065  1996 10654  6238  1005  1055  4373  2203  2003  4954  1012
  1010  3198  1996  9004  4497  1013  8843  2121  2065  2017  2064  5047
  1996 10654 15608  1012  1010  2298  2005 10654 15608  2008  2031 12538
 15695  1010  4408  2159  1010  1998  2024  3227  5379  1010  2065  1996
 10654  6238  2003  1037  2978  5376  2100  2012  2034  2043  2017  5047
  2032  1013  2014  1010  2123  1005  1056  4737  2008  2003  2025  1037
  2919  2518  1012  1010  5454  1037 10654  6238  2008  2017  2066  1012
   102], shape=(85,), dtype=int32)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cls</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">([</span><span class="s">'[CLS]'</span><span class="p">])]</span><span class="o">*</span><span class="n">sentence1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">input_word_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">cls</span><span class="p">,</span> <span class="n">sentence1</span><span class="p">,</span> <span class="n">sentence2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">input_word_ids</span><span class="p">.</span><span class="n">to_tensor</span><span class="p">())</span>
</code></pre></div></div>

<p><img src="/assets/images/2022-05-03-BERT-Analysis_files/2022-05-03-BERT-Analysis_14_0.png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_word_ids</span><span class="p">).</span><span class="n">to_tensor</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">input_mask</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.collections.QuadMesh at 0x7faa562b40f0&gt;
</code></pre></div></div>

<p><img src="/assets/images/2022-05-03-BERT-Analysis_files/2022-05-03-BERT-Analysis_15_1.png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">type_cls</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">cls</span><span class="p">)</span>
<span class="n">type_s1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sentence1</span><span class="p">)</span>
<span class="n">type_s2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">sentence2</span><span class="p">)</span>
<span class="n">input_type_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">type_cls</span><span class="p">,</span> <span class="n">type_s1</span><span class="p">,</span> <span class="n">type_s2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="n">to_tensor</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">input_type_ids</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.collections.QuadMesh at 0x7faa56299320&gt;
</code></pre></div></div>

<p><img src="/assets/images/2022-05-03-BERT-Analysis_files/2022-05-03-BERT-Analysis_16_1.png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode_sentence</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
   <span class="n">tokens</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
   <span class="n">tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">'[SEP]'</span><span class="p">)</span>
   <span class="k">return</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">bert_encode</span><span class="p">(</span><span class="n">glue_dict</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
  <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">glue_dict</span><span class="p">[</span><span class="s">"Sentence_1"</span><span class="p">])</span>
  
  <span class="n">sentence1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ragged</span><span class="p">.</span><span class="n">constant</span><span class="p">([</span>
      <span class="n">encode_sentence</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">glue_dict</span><span class="p">[</span><span class="s">"Sentence_1"</span><span class="p">])])</span>
  <span class="n">sentence2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ragged</span><span class="p">.</span><span class="n">constant</span><span class="p">([</span>
      <span class="n">encode_sentence</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
       <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">glue_dict</span><span class="p">[</span><span class="s">"Sentence_2"</span><span class="p">])])</span>

  <span class="n">cls</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">convert_tokens_to_ids</span><span class="p">([</span><span class="s">'[CLS]'</span><span class="p">])]</span><span class="o">*</span><span class="n">sentence1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">input_word_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">cls</span><span class="p">,</span> <span class="n">sentence1</span><span class="p">,</span> <span class="n">sentence2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">input_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_word_ids</span><span class="p">).</span><span class="n">to_tensor</span><span class="p">()</span>

  <span class="n">type_cls</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">cls</span><span class="p">)</span>
  <span class="n">type_s1</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sentence1</span><span class="p">)</span>
  <span class="n">type_s2</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">sentence2</span><span class="p">)</span>
  <span class="n">input_type_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span>
      <span class="p">[</span><span class="n">type_cls</span><span class="p">,</span> <span class="n">type_s1</span><span class="p">,</span> <span class="n">type_s2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="n">to_tensor</span><span class="p">()</span>

  <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s">'input_word_ids'</span><span class="p">:</span> <span class="n">input_word_ids</span><span class="p">.</span><span class="n">to_tensor</span><span class="p">(),</span>
      <span class="s">'input_mask'</span><span class="p">:</span> <span class="n">input_mask</span><span class="p">,</span>
      <span class="s">'input_type_ids'</span><span class="p">:</span> <span class="n">input_type_ids</span><span class="p">}</span>

  <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_train</span> <span class="o">=</span> <span class="n">bert_encode</span><span class="p">(</span><span class="n">df3</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="c1">#df_labels = df3['Relevancy_Score'].div(5)
</span>
<span class="n">df_test</span> <span class="o">=</span> <span class="n">bert_encode</span><span class="p">(</span><span class="n">df3_test</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">df_test_labels</span> <span class="o">=</span> <span class="n">df3_test</span><span class="p">[</span><span class="s">'Relevancy_Score'</span><span class="p">].</span><span class="n">div</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">df_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

KeyError                                  Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2897             try:
-&gt; 2898                 return self._engine.get_loc(casted_key)
   2899             except KeyError as err:


pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()


pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()


pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()


pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()


KeyError: 'Sentence_1'


The above exception was the direct cause of the following exception:


KeyError                                  Traceback (most recent call last)

&lt;ipython-input-29-1e8639691852&gt; in &lt;module&gt;()
----&gt; 1 df_train = bert_encode(df3, tokenizer)
      2 #df_labels = df3['Relevancy_Score'].div(5)
      3 
      4 df_test = bert_encode(df3_test, tokenizer)
      5 df_test_labels = df3_test['Relevancy_Score'].div(5)


&lt;ipython-input-27-02cbe3d43c41&gt; in bert_encode(glue_dict, tokenizer)
      6 
      7 def bert_encode(glue_dict, tokenizer):
----&gt; 8   num_examples = len(glue_dict["Sentence_1"])
      9 
     10   sentence1 = tf.ragged.constant([


/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in __getitem__(self, key)
   2904             if self.columns.nlevels &gt; 1:
   2905                 return self._getitem_multilevel(key)
-&gt; 2906             indexer = self.columns.get_loc(key)
   2907             if is_integer(indexer):
   2908                 indexer = [indexer]


/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2898                 return self._engine.get_loc(casted_key)
   2899             except KeyError as err:
-&gt; 2900                 raise KeyError(key) from err
   2901 
   2902         if tolerance is not None:


KeyError: 'Sentence_1'
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">df_train</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">key</span><span class="p">:</span><span class="mi">15</span><span class="n">s</span><span class="si">}</span><span class="s"> shape: </span><span class="si">{</span><span class="n">value</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'df_labels shape: </span><span class="si">{</span><span class="n">df_labels</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input_word_ids  shape: (2396, 125)
input_mask      shape: (2396, 125)
input_type_ids  shape: (2396, 125)
df_labels shape: (2396,)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">json</span>

<span class="n">bert_config_file</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">gs_folder_bert</span><span class="p">,</span> <span class="s">"bert_config.json"</span><span class="p">)</span>
<span class="n">config_dict</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="n">gfile</span><span class="p">.</span><span class="n">GFile</span><span class="p">(</span><span class="n">bert_config_file</span><span class="p">).</span><span class="n">read</span><span class="p">())</span>

<span class="n">bert_config</span> <span class="o">=</span> <span class="n">bert</span><span class="p">.</span><span class="n">configs</span><span class="p">.</span><span class="n">BertConfig</span><span class="p">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">config_dict</span><span class="p">)</span>

<span class="n">config_dict</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'attention_probs_dropout_prob': 0.1,
 'hidden_act': 'gelu',
 'hidden_dropout_prob': 0.1,
 'hidden_size': 768,
 'initializer_range': 0.02,
 'intermediate_size': 3072,
 'max_position_embeddings': 512,
 'num_attention_heads': 12,
 'num_hidden_layers': 12,
 'type_vocab_size': 2,
 'vocab_size': 30522}
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(bert.bert_models)
bert_classifier, bert_encoder = bert.bert_models.classifier_model(
   bert_config, num_labels=1)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;module 'official.nlp.bert.bert_models' from '/usr/local/lib/python3.6/dist-packages/official/nlp/bert/bert_models.py'&gt;
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf.keras.utils.plot_model(bert_classifier, show_shapes=True, dpi=48)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>glue_batch = {key: val[:10] for key, val in df_train.items()}

bert_classifier(
    glue_batch, training=True
).numpy()
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[-0.17102675],
       [ 0.07586445],
       [ 0.01797828],
       [-0.19046766],
       [-0.06210539],
       [-0.09033417],
       [ 0.01831295],
       [-0.08006046],
       [-0.22937882],
       [-0.03147416]], dtype=float32)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf.keras.utils.plot_model(bert_encoder, show_shapes=True, dpi=48)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[print(i.shape, i.dtype) for i in bert_classifier.inputs]
[print(o.shape, o.dtype) for o in bert_classifier.outputs]
[print(l.name, l.input_shape, l.dtype) for l in bert_classifier.layers]
bert_classifier.summary()
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(None, None) &lt;dtype: 'int32'&gt;
(None, None) &lt;dtype: 'int32'&gt;
(None, None) &lt;dtype: 'int32'&gt;
(None, 1) &lt;dtype: 'float32'&gt;
input_word_ids [(None, None)] int32
input_mask [(None, None)] int32
input_type_ids [(None, None)] int32
transformer_encoder [(None, None), (None, None), (None, None)] float32
dropout_1 (None, 768) float32
classification (None, 768) float32
Model: "bert_classifier"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_word_ids (InputLayer)     [(None, None)]       0                                            
__________________________________________________________________________________________________
input_mask (InputLayer)         [(None, None)]       0                                            
__________________________________________________________________________________________________
input_type_ids (InputLayer)     [(None, None)]       0                                            
__________________________________________________________________________________________________
transformer_encoder (Transforme [(None, None, 768),  109482240   input_word_ids[0][0]             
                                                                 input_mask[0][0]                 
                                                                 input_type_ids[0][0]             
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 768)          0           transformer_encoder[0][1]        
__________________________________________________________________________________________________
classification (Classification) (None, 1)            769         dropout_1[0][0]                  
==================================================================================================
Total params: 109,483,009
Trainable params: 109,483,009
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>checkpoint = tf.train.Checkpoint(model=bert_encoder)
checkpoint.restore(
    os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe309295a20&gt;
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Set up epochs and steps
epochs = 2
batch_size = 32
eval_batch_size = 32

train_data_size = len(df_labels)
steps_per_epoch = int(train_data_size / batch_size)
num_train_steps = steps_per_epoch * epochs
warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)

# creates an optimizer with learning rate schedule
optimizer = nlp.optimization.create_optimizer(
    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>type(optimizer)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>official.nlp.optimization.AdamWeightDecay
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>metric = [tf.keras.metrics.Accuracy('accuracy', dtype=tf.float32)]
loss = tf.keras.losses.MeanAbsoluteError()

bert_classifier.compile(
    optimizer=optimizer,
    loss=loss,
    metrics=['mse', 'mae', 'mape', 'cosine_similarity','accuracy'])

bert_classifier.fit(
      df_train, df_labels,
      batch_size=32,
      epochs=epochs)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/2
75/75 [==============================] - 3287s 44s/step - loss: 0.3301 - mse: 0.1647 - mae: 0.3301 - mape: 26140062.6908 - cosine_similarity: 0.8531 - accuracy: 0.0810
Epoch 2/2
75/75 [==============================] - 3245s 43s/step - loss: 0.1562 - mse: 0.0406 - mae: 0.1562 - mape: 11219197.4079 - cosine_similarity: 0.8746 - accuracy: 0.1360





&lt;tensorflow.python.keras.callbacks.History at 0x7fe3034ba908&gt;
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export_dir='./saved_model2'
tf.saved_model.save(bert_classifier, export_dir=export_dir)

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WARNING:absl:Found untraced functions such as self_attention_layer_call_fn, self_attention_layer_call_and_return_conditional_losses, attention_output_layer_call_fn, attention_output_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn while saving (showing 5 of 840). These functions will not be directly callable after loading.
WARNING:absl:Found untraced functions such as self_attention_layer_call_fn, self_attention_layer_call_and_return_conditional_losses, attention_output_layer_call_fn, attention_output_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn while saving (showing 5 of 840). These functions will not be directly callable after loading.


INFO:tensorflow:Assets written to: ./saved_model2/assets


INFO:tensorflow:Assets written to: ./saved_model2/assets
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bert_classifier.evaluate(df_test,df_test_labels)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>19/19 [==============================] - 172s 9s/step - loss: 0.1405 - mse: 0.0355 - mae: 0.1405 - mape: 12436080.0000 - cosine_similarity: 0.8980 - accuracy: 0.1405





[0.140456423163414,
 0.03549607843160629,
 0.140456423163414,
 12436080.0,
 0.8979933261871338,
 0.14046822488307953]
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>my_examples = bert_encode(
    glue_dict = {
        'Sentence_1':[
            'The rain in Spain falls mainly on the plain.',
            'Look I fine tuned BERT.',
            'I am alive.'],
        'Sentence_2':[
            'It mostly rains on the flat lands of Spain.',
            'Is it working? This does not match.',
            "I am alive."]
    },
    tokenizer=tokenizer)
result = bert_classifier(my_examples, training=False)

print(result)

#result = tf.argmax(result).numpy()

array = result.numpy()

def normalize(value):
	normalized = (value + 1) / (2);
	return normalized;


for i in array:
  x = normalize(i)
  print()
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf.Tensor(
[[0.6340052]
 [0.1166743]
 [0.7393124]], shape=(3, 1), dtype=float32)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!pip install h5py
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)
Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)
Requirement already satisfied: numpy&gt;=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.19.5)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bert_classifier.save("./model.h5")
print("Saved model to disk")
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Saved model to disk
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from google.colab import drive
drive.mount('/content/drive')
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mounted at /content/drive
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>reloaded = tf.saved_model.load(export_dir)
reloaded_result = reloaded([my_examples['input_word_ids'],
                            my_examples['input_mask'],
                            my_examples['input_type_ids']], training=False)

original_result = bert_classifier(my_examples, training=False)

# The results are (nearly) identical:
print(original_result.numpy())
print()
print(reloaded_result.numpy())
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0.6340052]
 [0.1166743]
 [0.7393124]]

[[0.63400537]
 [0.11667421]
 [0.7393121 ]]
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uploaded2 = files.upload()


predict = pd.read_csv(io.BytesIO(uploaded2['valtest.csv']))

predicter = bert_encode(glue_dict={"Sentence_1":predict['Sentence_1'],'Sentence_2': predict["Sentence_2"]}, tokenizer = tokenizer)

solutions = bert_classifier.predict(predicter)
</code></pre></div></div>

<p><input type="file" id="files-978f0911-aea2-45bc-b283-f1a2353214f3" name="files[]" multiple="" disabled="" style="border:none" /></p>
<output id="result-978f0911-aea2-45bc-b283-f1a2353214f3">
 Upload widget is only available when the cell has been executed in the
 current browser session. Please rerun this cell to enable.
 </output>
<script src="/nbextensions/google.colab/files.js"></script>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Saving valtest.csv to valtest.csv
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#a = (df.abs())

#b = normalized_predict

diff_pred = (solutions.tolist())

#print(diff_pred)

new_solutions = []
for i in diff_pred:
  for x in i:
    new_solutions.append(x)

ans = pd.Series(new_solutions)-predict['Relevancy_Score'].div(5)

print(ans.abs().describe().transpose())


</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>count    750.000000
mean       0.126923
std        0.096725
min        0.000204
25%        0.052139
50%        0.108334
75%        0.178874
max        0.557846
dtype: float64
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' 

map_name_to_handle = {
    'bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',
    'bert_en_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',
    'bert_multi_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',
    'small_bert/bert_en_uncased_L-2_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-2_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-2_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-2_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-4_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-4_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-4_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-6_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-6_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-6_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-6_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-8_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-8_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-8_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-8_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-10_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-10_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-10_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-10_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',
    'small_bert/bert_en_uncased_L-12_H-128_A-2':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',
    'small_bert/bert_en_uncased_L-12_H-256_A-4':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',
    'small_bert/bert_en_uncased_L-12_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',
    'small_bert/bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',
    'albert_en_base':
        'https://tfhub.dev/tensorflow/albert_en_base/2',
    'electra_small':
        'https://tfhub.dev/google/electra_small/2',
    'electra_base':
        'https://tfhub.dev/google/electra_base/2',
    'experts_pubmed':
        'https://tfhub.dev/google/experts/bert/pubmed/2',
    'experts_wiki_books':
        'https://tfhub.dev/google/experts/bert/wiki_books/2',
    'talking-heads_base':
        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',
}

map_model_to_preprocess = {
    'bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'bert_en_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/2',
    'small_bert/bert_en_uncased_L-2_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-2_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-2_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-2_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-4_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-4_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-4_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-6_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-6_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-6_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-6_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-8_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-8_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-8_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-8_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-10_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-10_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-10_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-10_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-12_H-128_A-2':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-12_H-256_A-4':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-12_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'small_bert/bert_en_uncased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'bert_multi_cased_L-12_H-768_A-12':
        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/2',
    'albert_en_base':
        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',
    'electra_small':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'electra_base':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'experts_pubmed':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'experts_wiki_books':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
    'talking-heads_base':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',
}

tfhub_handle_encoder = map_name_to_handle[bert_model_name]
tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]

print(f'BERT model selected           : {tfhub_handle_encoder}')
print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')


</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1
Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#Using the PREPROCESSING Model
bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
text_test = ['this is such an amazing movie!', 'I am Alive. I am Indebted','Praise to God almighty']
text_preprocessed = bert_preprocess_model(df2.Sentence_1)

print(text_preprocessed)

print(f'Keys       : {list(text_preprocessed.keys())}')
print(f'Shape      : {text_preprocessed["input_word_ids"].shape}')
print(f'Word Ids   : {text_preprocessed["input_word_ids"][0, :12]}')
print(f'Input Mask : {text_preprocessed["input_mask"][0, :12]}')
print(f'Type Ids   : {text_preprocessed["input_type_ids"][0, :12]}')

#Using the BERT Model
bert_model = hub.KerasLayer(tfhub_handle_encoder)
bert_results = bert_model(text_preprocessed)

print(bert_results)
print(f'Loaded BERT: {tfhub_handle_encoder}')
print(f'Pooled Outputs Shape:{bert_results["pooled_output"].shape}')
print(f'Pooled Outputs Values:{bert_results["pooled_output"][0, :12]}')
print(f'Sequence Outputs Shape:{bert_results["sequence_output"].shape}')
print(f'Sequence Outputs Values:{bert_results["sequence_output"][0, :12]}')
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-6-d818c6d3088a&gt; in &lt;module&gt;()
      2 bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)
      3 text_test = ['this is such an amazing movie!', 'I am Alive. I am Indebted','Praise to God almighty']
----&gt; 4 text_preprocessed = bert_preprocess_model(df2.Sentence_1)
      5 
      6 print(text_preprocessed)


/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1010         with autocast_variable.enable_auto_cast_variables(
   1011             self._compute_dtype_object):
-&gt; 1012           outputs = call_fn(inputs, *args, **kwargs)
   1013 
   1014         if self._activity_regularizer:


/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py in call(self, inputs, training)
    235       result = smart_cond.smart_cond(training,
    236                                      lambda: f(training=True),
--&gt; 237                                      lambda: f(training=False))
    238 
    239     # Unwrap dicts returned by signatures.


/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     54       return true_fn()
     55     else:
---&gt; 56       return false_fn()
     57   else:
     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,


/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py in &lt;lambda&gt;()
    235       result = smart_cond.smart_cond(training,
    236                                      lambda: f(training=True),
--&gt; 237                                      lambda: f(training=False))
    238 
    239     # Unwrap dicts returned by signatures.


/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py in _call_attribute(instance, *args, **kwargs)
    666 
    667 def _call_attribute(instance, *args, **kwargs):
--&gt; 668   return instance.__call__(*args, **kwargs)
    669 
    670 


/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--&gt; 828       result = self._call(*args, **kwds)
    829       compiler = "xla" if self._experimental_compile else "nonXla"
    830       new_tracing_count = self.experimental_get_tracing_count()


/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    869       # This is the first call of __call__, so we have to initialize.
    870       initializers = []
--&gt; 871       self._initialize(args, kwds, add_initializers_to=initializers)
    872     finally:
    873       # At this point we know that the initialization is complete (or less


/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    724     self._concrete_stateful_fn = (
    725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 726             *args, **kwds))
    727 
    728     def invalid_creator_scope(*unused_args, **unused_kwds):


/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2967       args, kwargs = None, None
   2968     with self._lock:
-&gt; 2969       graph_function, _ = self._maybe_define_function(args, kwargs)
   2970     return graph_function
   2971 


/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3312     if self.input_signature is None or args is not None or kwargs is not None:
   3313       args, kwargs, flat_args, filtered_flat_args = \
-&gt; 3314           self._function_spec.canonicalize_function_inputs(*args, **kwargs)
   3315     else:
   3316       flat_args, filtered_flat_args = [None], []


/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in canonicalize_function_inputs(self, *args, **kwargs)
   2695 
   2696     if self._input_signature is None:
-&gt; 2697       inputs, flat_inputs, filtered_flat_inputs = _convert_numpy_inputs(inputs)
   2698       kwargs, flat_kwargs, filtered_flat_kwargs = _convert_numpy_inputs(kwargs)
   2699       return (inputs, kwargs, flat_inputs + flat_kwargs,


/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _convert_numpy_inputs(inputs)
   2755         raise TypeError("The output of __array__ must be an np.ndarray "
   2756                         "(got {} from {}).".format(type(a), type(value)))
-&gt; 2757       flat_inputs[index] = constant_op.constant(a)
   2758       filtered_flat_inputs.append(flat_inputs[index])
   2759       need_packing = True


/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    263   """
    264   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--&gt; 265                         allow_broadcast=True)
    266 
    267 


/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    274       with trace.Trace("tf.constant"):
    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
--&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
    277 
    278   g = ops.get_default_graph()


/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):
    300   """Implementation of eager constant."""
--&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)
    302   if shape is None:
    303     return t


/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum
     97   ctx.ensure_initialized()
---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)
     99 
    100 


ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sentence1_input = tf.keras.Input(shape=(), dtype=tf.string, name = "Sentence1")
sentence2_input = tf.keras.Input(shape=(), dtype=tf.string, name = "Sentence2")
relevancyinput = tf.keras.Input(shape=(1), name = "RelevancyScore")

sentencepreprocessing = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing1')

sentence1_preprocessing = sentencepreprocessing(sentence1_input)
sentence2_preprocessing = sentencepreprocessing(sentence2_input)

sentencebert = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='BERT_encoder1')

sentence1_bert = sentencebert(sentence1_preprocessing)
sentence2_bert = sentencebert(sentence2_preprocessing)


concatenate_all = tf.keras.layers.concatenate([relevancyinput, sentence1_bert["pooled_output"], sentence2_bert["pooled_output"]])
neta2 = tf.keras.layers.Dense(1024, activation=None, name='sts1')(concatenate_all)
neta3 = tf.keras.layers.Dense(768, activation=None, name='sts2')(neta2)
net = tf.keras.layers.Dense(1, activation=None, name='sts')(neta3)

model = tf.keras.Model(
    inputs=[sentence1_input, sentence2_input],
    outputs=[net],
)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def CosineSimilarity(a,b):
  #a**b/squareroot(summation of a^2)**squaeroot(summation of b^2)
  for i in a:
    for o im b:
      a *
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[print(i.shape, i.dtype) for i in model.inputs]
[print(o.shape, o.dtype) for o in model.outputs]
[print(l.name, l.input_shape, l.dtype) for l in model.layers]
model.summary()
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(None,) &lt;dtype: 'string'&gt;
(None,) &lt;dtype: 'string'&gt;
(None, 1) &lt;dtype: 'float32'&gt;
Sentence1 [(None,)] string
Sentence2 [(None,)] string
preprocessing1 None float32
BERT_encoder1 {'input_word_ids': (None, 128), 'input_mask': (None, 128), 'input_type_ids': (None, 128)} float32
concatenate_5 [(None, 512), (None, 512)] float32
sts1 (None, 1024) float32
sts2 (None, 1024) float32
sts (None, 768) float32
Model: "model_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Sentence1 (InputLayer)          [(None,)]            0                                            
__________________________________________________________________________________________________
Sentence2 (InputLayer)          [(None,)]            0                                            
__________________________________________________________________________________________________
preprocessing1 (KerasLayer)     {'input_word_ids': ( 0           Sentence1[0][0]                  
                                                                 Sentence2[0][0]                  
__________________________________________________________________________________________________
BERT_encoder1 (KerasLayer)      {'pooled_output': (N 28763649    preprocessing1[0][0]             
                                                                 preprocessing1[0][1]             
                                                                 preprocessing1[0][2]             
                                                                 preprocessing1[1][0]             
                                                                 preprocessing1[1][1]             
                                                                 preprocessing1[1][2]             
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 1024)         0           BERT_encoder1[0][5]              
                                                                 BERT_encoder1[1][5]              
__________________________________________________________________________________________________
sts1 (Dense)                    (None, 1024)         1049600     concatenate_5[0][0]              
__________________________________________________________________________________________________
sts2 (Dense)                    (None, 768)          787200      sts1[0][0]                       
__________________________________________________________________________________________________
sts (Dense)                     (None, 1)            769         sts2[0][0]                       
==================================================================================================
Total params: 30,601,218
Trainable params: 1,837,569
Non-trainable params: 28,763,649
__________________________________________________________________________________________________
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                                                                                                                                                                                                                      #Defining the loss function
loss = tf.keras.losses.MeanAbsoluteError()
metrics = tf.metrics.Accuracy()

#Defining the Optimizer
epochs = 4
steps_per_epoch = 5
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 3e-5
optimizer = optimization.create_optimizer(init_lr=init_lr,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')

#Loading the BERT Model and Training
model.compile(optimizer=optimizer,loss=loss,metrics=['mse', 'mae', 'mape', 'cosine_similarity','accuracy'])

print(df3["Relevancy_Score"])

sentence1_data = (df3.Sentence_1).astype("string")
relevancydata = pd.to_numeric((df3['Relevancy_Score']),errors='coerce')
normalized = relevancydata.div(5)
sentence2_data = (df3.Sentence_2).astype("string")
print(normalized)
print(type(sentence1_data[1]))

model.fit(
    {"Sentence1": sentence1_data, "Sentence2": sentence2_data}, y = normalized, epochs=epochs )

print(f'Training model with {tfhub_handle_encoder}')
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>776     4.0
1424    3.0
227     4.7
2402    0.8
104     5.0
       ... 
409     4.4
1623    2.8
115     5.0
288     4.6
2510    0.4
Name: Relevancy_Score, Length: 2305, dtype: float64
776     0.80
1424    0.60
227     0.94
2402    0.16
104     1.00
        ... 
409     0.88
1623    0.56
115     1.00
288     0.92
2510    0.08
Name: Relevancy_Score, Length: 2305, dtype: float64
&lt;class 'str'&gt;
Epoch 1/4
73/73 [==============================] - 312s 4s/step - loss: 0.4590 - mse: 0.3090 - mae: 0.4590 - mape: 98865381.2973 - cosine_similarity: 0.9011 - accuracy: 0.0438
Epoch 2/4
73/73 [==============================] - 306s 4s/step - loss: 0.4504 - mse: 0.2993 - mae: 0.4504 - mape: 89167020.2162 - cosine_similarity: 0.9108 - accuracy: 0.0476
Epoch 3/4
73/73 [==============================] - 308s 4s/step - loss: 0.4469 - mse: 0.2979 - mae: 0.4469 - mape: 92691536.1622 - cosine_similarity: 0.9073 - accuracy: 0.0570
Epoch 4/4
73/73 [==============================] - 305s 4s/step - loss: 0.4534 - mse: 0.3041 - mae: 0.4534 - mape: 95444199.7838 - cosine_similarity: 0.9046 - accuracy: 0.0450
Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>


df3_test = df2_test.dropna()

print(df3_test.describe().transpose())

sentence1_test = df3_test["Sentence_1"]
sentence2_test = df3_test["Sentence_2"]
relevancy_test = df3_test["Relevancy_Score"]
normalized_test = relevancy_test.div(5)
metric = tf.metrics.CosineSimilarity()
losses = model.evaluate({"Sentence1": sentence1_test, "Sentence2": sentence2_test}, {"sts": normalized_test})

print(f'Loss: {losses}')

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-1-9144b8188c9a&gt; in &lt;module&gt;()
      2 
      3 
----&gt; 4 df3_test = df2_test.dropna()
      5 
      6 print(df3_test.describe().transpose())


NameError: name 'df2_test' is not defined
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uploaded2 = files.upload()

predict = pd.read_csv(io.BytesIO(uploaded2['predict.csv']))


sentence1_predict = predict["Sentence_1"]
sentence2_predict = predict["Sentence_2"]
relevancy_predict = predict["Relevancy_Score"]
normalized_predict = relevancy_predict.div(5)


solutions = model.predict({"Sentence1": sentence1_predict, "Sentence2": sentence2_predict})
</code></pre></div></div>

<p><input type="file" id="files-96231d17-f920-44cd-b706-66c27ac114d1" name="files[]" multiple="" disabled="" style="border:none" /></p>
<output id="result-96231d17-f920-44cd-b706-66c27ac114d1">
 Upload widget is only available when the cell has been executed in the
 current browser session. Please rerun this cell to enable.
 </output>
<script src="/nbextensions/google.colab/files.js"></script>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Saving predict.csv to predict (1).csv
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
newList = []
for x in solutions:
    newList.append(x*5)

df = pd.DataFrame(solutions)
fileName = 'prdict.csv'
df.to_csv(fileName)

print(df)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>           0
0  -0.296717
1  -0.083558
2   0.316203
3  -0.035383
4   0.340398
5   0.672012
6   0.557940
7   0.458394
8   0.027319
9   0.368085
10  0.478323
11  0.515013
12  1.077829
13  0.733209
14  0.935209
15  0.316129
16  0.044420
17  0.027349
18  0.295618
19  0.381707
20  0.388599
21  0.310701
22  0.688457
23 -0.062880
24  0.946695
25 -0.148819
26  0.647570
27  0.811228
28  0.595808
29  0.022950
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a = (df.abs())

b = normalized_predict

diff_pred = (b).subtract(a)

print(diff_pred[0].abs())

diff_pred[0].abs().describe().transpose()

print((1 - diff_pred[0].abs()).sum())
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-1-a4cb93cb6cf5&gt; in &lt;module&gt;()
----&gt; 1 a = (df.abs())
      2 
      3 b = normalized_predict
      4 
      5 diff_pred = (b).subtract(a)


NameError: name 'df' is not defined
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dataset_name = 'imdb'
saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))
model.save(saved_model_path, include_optimizer=False)
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading.
WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading.
</code></pre></div></div>


        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2022-05-03T00:00:00+03:00">May 3, 2022</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Bert+Analysis%20http%3A%2F%2Flocalhost%3A4000%2FBERT-Analysis%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2FBERT-Analysis%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2FBERT-Analysis%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/Decision-Trees/" class="pagination--pager" title="Decision Trees
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/Decision-Trees/" rel="permalink">Decision Trees
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">#Importing dependancies
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import DecisionTreeClassifier...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/Convex-Optimization/" rel="permalink">Convex Optimization
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          less than 1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The best solutions for each scenario. How better can we improve a system. One way of tackling this is through simulation.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/Monte-Carlo-Simulations/" rel="permalink">Monte Carlo Simulations
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">What are Monte Carlo Simulation?

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/Monte-Carlo-Methods-2/" rel="permalink">Monte Carlo Methods 2
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Now, lets talk about something else apart from the binomial distribution. Say you have a circle, and you want to drop balls over it. One by one you drop a ba...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form onsubmit="return googleCustomSearchExecute();" id="cse-search-box-form-id" role="search">
    <label class="sr-only" for="cse-search-input-box-id">
      Enter your search term...
    </label>
    <input type="search" id="cse-search-input-box-id" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results">
    <gcse:searchresults-only></gcse:searchresults-only>
  </div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Mac Maxwell. Designed by <a href="https://jekyllrb.com" rel="nofollow">Rarelane Analytics</a></div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>


<script>
  (function () {
    var cx = '';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();

  function googleCustomSearchExecute() {
    var input = document.getElementById('cse-search-input-box-id');
    var element = google.search.cse.element.getElement('searchresults-only0');
    if (input.value == '') {
      element.clearAllResults();
    } else {
      element.execute(input.value);
    }
    return false;
  }

  
</script>







  </body>
</html>
