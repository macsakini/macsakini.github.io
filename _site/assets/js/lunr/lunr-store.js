var store = [{
        "title": "Algorithms Insertion Sort",
        "excerpt":"Algorithms - Insertion Sort  import java.util.Arrays;  class insertionsort {     static int[] value = {14,33,27,35,10};     public static void main(String[] args){         int holePosition;         int valueToInsert;          for(int i = 1; i &lt; value.length; i++ ){             valueToInsert = value[i];                          holePosition = i;              while(holePosition &gt; 0 &amp;&amp; value[holePosition - 1] &gt; valueToInsert){                                  value[holePosition] = value[holePosition - 1];                                  holePosition = holePosition - 1;                  System.out.println(Arrays.toString(value));                              }              value[holePosition] = valueToInsert;         }          System.out.println(Arrays.toString(value));     }     }  ","categories": [],
        "tags": [],
        "url": "/Algorithms-Insertion-Sort/",
        "teaser": null
      },{
        "title": "Algorithms Merge Sort",
        "excerpt":"Algorithms - Merge Sort  import java.util.Arrays;  class mergesort {     static int[] value = {14,33,27,19,42,44,78,6};              static int n = value.length;     public static void main(String[] args){         mergesort(value, n);     }      public static void mergesort(int[] arr, int n){         if(n == 1){             return;         }         // System.out.println((n));          // System.out.println(Arrays.toString(arr));          int mid  = n/2;          int[] l1 = new int[mid];                  int[] l2 = new int[n - mid];                   for (int i = 0; i &lt; mid; i++) {             l1[i] = arr[i];         }          for (int i = mid; i &lt; n; i++) {             l2[i - mid] = arr[i];         }          // System.out.println(Arrays.toString(l1));         // System.out.println(Arrays.toString(l2));          mergesort(l1, mid);          mergesort(l2, n-mid);          merge(l1,l2);       }      public static void merge(int[] a, int[] b){         int[] c = new int[0];          System.out.println(a.length);                   while(a.length &gt; 0 &amp;&amp; b.length &gt; 0){             if(a[0] &gt; b[0]){                 c[c.length] = (b[0]);                 Arrays.asList(b).remove(b[0]);             }else{                 c[c.length] = a[0];                 Arrays.asList(a).remove(a[0]);             }         }          while(a.length &gt; 0 ){             c[c.length] = a[0];             Arrays.asList(a).remove(a[0]);         }          while(b.length &gt; 0 ){             c[c.length] = (b[0]);             Arrays.asList(b).remove(b[0]);         }          System.out.println(Arrays.toString(c));      }      }  ","categories": [],
        "tags": [],
        "url": "/Algorithms-Merge-Sort/",
        "teaser": null
      },{
        "title": "Algorithms Selection Sort",
        "excerpt":"Algorithms - Select Sort   import java.util.Arrays;  class selectsort {     static int[] value = {14,33,27,245,10,35,19,42,44,78,6};     public static void main(String[] args){         int n = value.length;          System.out.println(Arrays.toString(value));                  for(int i = 0; i &lt; (n - 1); i++){             int min = i;              for(int j = (i + 1); j &lt; n; j++){                 System.out.println(Arrays.toString(value));                  if (value[j] &lt; value[min]) {                     min = j;                 }             }             System.out.println(Arrays.toString(value));              if (min != i){                 swap(min, i);             }         }         System.out.println(Arrays.toString(value));      }         public static String swap(int i, int j){         int temp = value[i];         value[i] = value[j];         value[j] = temp;         return\"success\";     } }  ","categories": [],
        "tags": [],
        "url": "/Algorithms-Selection-Sort/",
        "teaser": null
      },{
        "title": "Algorithms Bubble Sort",
        "excerpt":"Algorithms - Bubble Sort  import java.util.Arrays;  class BubbleSort{     //static ArrayList&lt;Integer&gt; value = new ArrayList&lt;Integer&gt;(Arrays.asList(14,33,27,35,10));     static int[] value = {14,33,27,35,10};          public static void main(String[] args){          System.out.println(Arrays.toString(value));         for(int i = 0; i &lt; (value.length - 1); i++){             Boolean swapped = false;             for(int j = 0; j &lt; (value.length - 1); j++){                 System.out.println(j);                 if(value[j] &gt; value[j+1]){                     swap(j, j+1);                     swapped = true;                     System.out.println(Arrays.toString(value));                 }             }             System.out.println(swapped);             if(swapped == false){                 break;             }         }          System.out.println(Arrays.toString(value));     }      public static String swap(int i, int j){         int temp = value[i];         value[i] = value[j];         value[j] = temp;         return\"success\";     } }  ","categories": [],
        "tags": [],
        "url": "/Algorithms-Bubble-Sort/",
        "teaser": null
      },{
        "title": "Ds Stacks And Queues",
        "excerpt":"Data Structures - Stacks and Queues in Java.   Stacks  You are probably wondering, what are stacks. Well Stacks are stacks üòõ. It is in the name right. Well a stack is how you put your plates. Or how you play that cup game. You just put things over each other.   The main principle behind stacks is LIFO: Last In First Out.  The main two actions a stack performs is pushing and poping. You push something into the stack and you pop something out of the stack.   Java has a built in stack framework. Find below an image showcasing this characteristic.   Image Goes HERE‚Ä¶.   Java implements it quite nicely, create a new stack called games. To add items into the games stack just use the add function. To remove stuff out of the games stack just use pop.   The second example shows a character stack list. Here am justing adding single characters such as ‚ÄòA‚Äô, ‚ÄòB‚Äô and so on.  import java.util.Stack;  public class stacky {     public static void main(String[] args){         Stack&lt;String&gt; games = new Stack&lt;String&gt;();          games. add(\"Call of Duty\");          games.add(\"Super Monkey Ball\");          games.add(\"Guitar Hero\");          System.out.println(games);          //if you would like to get topmost one          System.out.println(games.pop());          //to get second from top         System.out.println(games.pop());          // to get remaining stack after two pop operations         System.out.println(games);          //to check the first item on the stack          System.out.println(games.peek());          // Y         // B         // R          Stack&lt;Character&gt; tower = new Stack&lt;Character&gt;();          tower.add('R');         tower.add('B');         tower. add('R');          System.out.println(tower.empty());         System.out.println(tower.get(1));         System.out.println(tower.set(1, 'P'));         System.out.println(tower.size());        } }   The queue is just stacks wacky brother. Same concept except the queue is an actual groceries shop queue or the line at the hospital. So whoever walked in first, gets served first. This is an interesting principle called FIFO: First In First Out.   The actions that describe a queue at best are enqueue and dequeue. However, Java makes it simple, you add or poll a queue. Make sure to copy paste the code and try it out.   Image does here   Queues  import java.util.LinkedList; import java.util.Queue;  public class queueish {     public static void main(String[] args){         //Example one         Queue&lt;String&gt; bbq = new LinkedList&lt;String&gt;();          bbq.add(\"Jeff\");          bbq.add(\"Tyrique\");          bbq.add(\"Susan\");                  bbq.peek();          System.err.println(bbq);          //Jeff         bbq.poll();         //Tyrqique         bbq.poll();         //Susan         System.out.println(bbq.poll());          //Example two          Queue&lt;String&gt; q = new LinkedList&lt;String&gt;();          q.add(\"A\");         q.add(\"B\");         q.add(\"C\");          q.poll();          System.out.println(q);          System.out.println(q.size());          System.out.println(q.contains(\"A\"));          System.out.println(q.toArray()[1]);      }  }  ","categories": [],
        "tags": [],
        "url": "/DS-Stacks-and-Queues/",
        "teaser": null
      },{
        "title": "Ds Linked Lists Arrays",
        "excerpt":"Data Structures - Linked Lists, Arrays, Hashmap   This is a collection of some of the most fundamental data structures and functions done in Java.   Each file in this repository is a snippet of the java function assigned. The name of the repository is also closely related to the task at hand. i.e getset.java concernes getters and setters.   Linked Lists  import java.util.Iterator; import java.util.LinkedList;  class Linkylist {     public static void main(String[] args){         LinkedList&lt;String&gt; linky = new LinkedList&lt;String&gt;();          //you can add elements to the LinkedAList          linky.add(\"Rob\");         linky.add(\"animal\");         linky.add(\"Hose\");          linky.getFirst();         //linky.remove();          System.out.println(linky);          LinkedList&lt;Integer&gt; numlinky = new LinkedList&lt;Integer&gt;();          numlinky.add(1);         numlinky.add(78);         numlinky.add(456);          System.out.println(numlinky.get(2));          //iterating through the linkedlist to find a number          Iterator it = numlinky.iterator();          while(it.hasNext()){             if((int) it.next() == 78){                 System.out.println(\"We have found 78\");             }         }               } }  Arrays   import java.util.ArrayList; import java.util.List;  class arrayish {     public static void main(String[] args){         List&lt;Integer&gt; a = new ArrayList&lt;Integer&gt;();          a.add(10);         a.add(20);          List &lt;String&gt; b = new ArrayList&lt;String&gt;();          for(int i : a){             System.out.println(String.valueOf(i));             b.add(String.valueOf(i));         }          System.out.println(b);       } }  HashMaps and HashTables   import java.util.HashMap;  public class hashmapy {     public static void main(String[] args){         int a, b, c;         a = 1;         b = 2;         c = 3;          HashMap&lt;String,Integer&gt; happy = new HashMap&lt;&gt;();          happy.put(\"a\", a);         happy.put(\"b\", b);         happy.put(\"c\",c);           System.out.println(happy.get(\"c\"));     })  }   ","categories": [],
        "tags": [],
        "url": "/DS-Linked-Lists-Arrays/",
        "teaser": null
      },{
        "title": "Linear Algebra Series 2",
        "excerpt":"Linear Algebra Series 2 - Linear Independence and Dependence   I will try to avoid math, so test 1,   \\[A \\implies B \\, \\exists \\, x \\: \\in \\R\\]  And yes, it is working. This was just a test, dont copy that. Now lets get down to it.  First, order of business, is linear independence. This is a major concept in linear algebra but one the Brits seem to never understand ü§®. I‚Äôll describe both the formal and you versions for better understanding.   Formal Definition.   Linear independence. Say we have set of vectors \\(x_1, x_2 \\land x_3\\) in a linear subspace, they can only be termed independent if \\(\\exists \\, c_1, c_2, c_3\\) such that:   \\[c_1 * x_1 + c_2 * x_2 + c_3 * x_3 = 0\\]  \\(\\land \\: \\therefore\\) a null vector, where \\(c_1 = c_2 = c_3 = 0\\)   Linear dependence on the other hand is the same except atleast one scalar is non-zero.   Note Linearly dependent vectors can be written as a linear combination of linearly independent vectors.   Logical Definition.  ","categories": [],
        "tags": [],
        "url": "/Linear-Algebra-Series-2/",
        "teaser": null
      },{
        "title": "Modality And  seasonality On Security Prices",
        "excerpt":"Modality and Seasonality on Security Prices in the DOW   The data in the data folder is temporal and thus the models to be used are time series models. The analysis is not geared towards prediction but is focused on finding the right parameters and metrics that best decsribes these datasets. It is also geared by finding which of the time series approaches used in project is more powerful, compact and accurate compared to the other.    Figure 1.1 The daily returns of the security.   The following is a snippet of the exponential smoothing done in R language:   Exponential smoothing.     Exponential smoothing. This method works by averaging historical data and using those weights to create a more robust and noise-less trend compared to actual real-time data with high critical volatility. The methodology creates a safe space for predicting future values in a certain trend space. It reduces the effect of the trend on the movement and tries to predict future values.    Figure 1.2 Fitted ES model.   #Exponential smoothing model exponential &lt;- function(data){   summary(es(data,h=100, holdout=FALSE, silent=FALSE, alpha=.2))   } exponential(data_x)   ARIMA.     ARIMA. This is a model that is used in finding the flow of data using a regression-like model that is based on previous time stamps. It is a combination of AR or autoregressive and MA moving average models with a integration using differencing to make the non-stationary data stationary. It will be used to show, seasonality, trend and autocorrelation.  Figure 1.3 Arime model fitted on the data.   The following is a snippet of the ARIMA code done in the R language:  #AUTOMATIC ARIMA autoarima &lt;- function(data){   x &lt;- auto.arima(data)   print(checkresiduals(x))   print(summary(x))   autoplot(x) } autoarima(data_x)  #ARIMA MODEL OF CHOICE setarima &lt;- function(data){   x &lt;- Arima(data, order=c(1,1,1))   print(summary(x))   print(checkresiduals(x))   autoplot(x) } setarima(data_x)   Regression     Regression. Regression done on a time series perspective has to be dynamic and will show the level of correlation. Regular regression is done using the concerned variable to a relatable variable in same data space. In our scenario since these are parallel time-series, regression can be done directly to the trans-mutated date-time variable or it can be done against the other time-series and the greatest correlation of these can be determined using the coefficients.   A lot of the data gathered everyday is __time series data__. From individuals, to organizations there has been a fourth industrial revolution of data. Analytics is becoming part of everyday life and slowly by slowly it is being integrated into everyday life. From insurance using telematics to fitness using smart watches, these are just some of the applications that use data and data analysis.  ","categories": [],
        "tags": [],
        "url": "/Modality-and-Seasonality-on-Security-Prices/",
        "teaser": null
      },{
        "title": "Linear Algebra Series",
        "excerpt":"Linear Algebra Series 1 - Introduction   This particular blog post will cover at large the basics of linear algebra and how to understand it in thirty minutes. Cap. I would give me a yearüò¨.   We are gonna use a lot of Python for the computations so be ready to do some nice copy paste. The first section involves importing all necessary libraries into the workspace. In this case they are only two:ü•≤. Our star library for today is the SciPy library which will mostly deal with scientific calculations and as such scientific subjects, disciples, I mean disciplines, notations, et al‚Ä¶.   import numpy as np import scipy.linalg as sp  Follow by, creating a matrix using the array function by numpy. The formula remains standard üòä RC which is Roman Catholic which means Rows then Columns   a = np.array([[1,2,3],[4,5,6],[7,88,9]]) b = np.array([[1,2,3],[4,5,6],[7,8,9]])  Basic Operations on Matrices   Finding the determinant. NOTE:= The determinant is a scalar value.  Say we have a matrix A = [[a,b],[c,d]]: We find the detriminant using the formula \\(Det(A) = (a*d)-(c*b)\\). The determinant and this is not a formal definition, can be thought of as the ‚Äúvolume‚Äù of the space occupied by the matrix.   sp.det(a)  If the determinant is zero, this matrix becomes a singular matrix. Otherwise, it is just a regular square matrix. I say square as there exist other types of Matrices, but these will be discussed in other sections of the series.   B is an example singular matrix. Ensure to test with B as well, in place of A, just to get a feel of the concept.                  Next, we can find the norm of the matrix :=       A       . Similar to any vector, matrices have a norm which is just their magnitude (size).           The norm of the matrix can be derived using the norm function.   sp.norm(a)   The inverse of the matrix. As the word implies, is the alternative matrix in the opposite, say what, ‚Äúdirection‚Äù but orthogonally. Putting it in another way, interchange a and d, give b and c, negative signs and ‚Äúvwaalah‚Äù inverse. [[a,b],[c,d]] \\ \\Longrightarrow \\ [[d,-b], [-c,a]]. However, a more formal way of doing it is using the inv function provided by SciPy. Mathematicians, use a matrix of signs to find the inverse for bigger matrices. 3 x 3, 4 x 4 and the gang.   Note a singular matrix, does not have an inverse.  try:   print(sp.inv(a)) except:   print(\"This matrix is a singular matrix\")   Finding the Eigenvalues and Eigenvectors  This section just shows the way of finding the eigen‚Äôs using SciPy. However, I will add a link down below to an exclusive post, fully focused, on what these are, how the help and where to use them. They carry a major role in CS, Physics, Chemistry‚Ä¶ as such they are very important.   Finding the eigenvalues and eigenvectors.  sp.eig(a)  This specifically prints out the eigenvalues.  sp.eigvals(a)  Lets talk about matrix decomposition  There are so many ways of decomposing a matrix. Name a few, LU, QR, QZ, Polar, SVD and so much more. You are probably wondering, who are all these Chinese babies. However, i will disappoint you, these are decomposition styles. I will cover only the most important among these.   They all try to achieve the same task: make matrix computations easier. Think of decomposition as making matrices into legos. The matrix doesn‚Äôt loose its end value, we just break it into pieces, that when we remap together reform the broken down matrix. This happens so that they can be used in Ninjago and be used differently and more efficiently at various tasks. Imagine, 9 is a matrix, if I break it into 4 and 5 or even 2, 4 and 3, we havent really changed the 9. I have just broken it down or decomposed it into a number of values. Decomposition is somewhat similar, not direct, yes, but somewhat similar.   LU Decomposition  This is a pivoted LU decomposition.  lu = sp.lu(a)  print(lu[0], \"\\n\") print(lu[1], \"\\n\") print(lu[2], \"\\n\")   sp.lu_factor(a)  Singular Value Decomposition  This is an SVD.  svd = sp.svd(a)  print(svd[0], \"\\n\") print(svd[1], \"\\n\") print(svd[2], \"\\n\")  sp.svdvals(a)  Find the orthonormal basis for A using SVD  Basis and orthonormal shall be covered in the next post. Be sure to head there.  sp.orth(a)  Finds the orthonormal basis for the null_space using SVD  sp.null_space(a)  Polar Decomposition  sp.polar(a)  Matrix can also be considered as regular numbers with regular applications.  This definitively means, you can carry out operations similar to those on real and natural numbers without any siginificant changes. A number of the operations include:      Finding the exponential of the matrix.     sp.expm(a)           Finding the sine of the matrix.     sp.sinm(a)           Finding the cosine.     sp.cosm(a)           Finding the tangent of the matrix.     sp.tanm(a)           Finding the square root of the matrix.     sp.sqrtm(a)           And even applying a function to the matrix. For example, this is the dot product of the matrix. ```python print(sp.funm(a, lambda x: x*x))   print(‚Äú\\n‚Äù)   print(np.dot(a,a))   ## There is a whole lot of special matrices. However i will just build a few to demonstrate.  This creates a 3 by 3 hilbert matrix. Hilbert spaces have special characteristics coupled up with the Inner Product and the Banach Space and so on... but i will also discuss this in later sections and in full length. I mean, this is the introductory, you wouldn't want me on Banach Spaces, right. ```python sp.hilbert(3)  Hadamard matrix is also a special matrix. Some arab guy made it. Its mostly used in quantum computations due to its nature of just being 1‚Äôs and -1‚Äôs, big deal, whoop. ITS A VERY BIG DEAL.ü´° .They are used for the opening and closing of logic gates in quantum computers coupled with superposition which i will not even dare mention.  sp.hadamard(4)   This creates a discrete fourier transform matrix. FO-RII-EEEH.  sp.dft(3)   This creates a helmert matrix. Its kind of used with the hadamard, but do some research.  sp.helmert(3)    That mostly sums it up. This is an introductory session on using the scipy library to evaluate some of Linear Algebra‚Äôs most common functions. In the next, sections, I will get in deeper into the aforementioned topics, be there. Stay Tuned.üòå  ","categories": [],
        "tags": [],
        "url": "/Linear-Algebra-Series/",
        "teaser": null
      },{
        "title": "Option Pricing",
        "excerpt":"Option Analysis and Prediction using Fast Fourier Transform and Levy Alpha Stable Distributions   This project uses a number of concepts.           Black Scholes model.            Capital Assets Pricing Model (CAPM).            Levy Alpha Stable Distributions.            Market Model.            Fourier and Inverse Fourier Transformations.       The files of the project are all over the place but each file deals with a certain topic. I have also included a docx file explaining the entire theorem in the data folder.   However the following concept can be critical to understand.          Options are regularly proced using the BS model.            Volatility in stock markets and securities are regularly functions of normal distributions according to a number of models.        Figure 1.1 CAPM Model           Technical analysis incorporates normal distributions. This can also be seen in the thrid graph in the image above. The residuals are trying to get fit under a normal curve.            Beta Coefficient explains risk in a regression model of market prices with underlying asset.            Beta distributions can take different forms depending on the shape and scale parameters assigned to it. Bet coefficients are not related to beta distributions            Prior analysis has shown that in some cases fitting a different distribution similar to the normal explains better the fractals and risk numbers in a market.            Levy Distributions are extracts of the stable distributions. According to wikipedia, beta distributions are not \"‚Äùnot analytically expressible, except for some parameter values‚Äù‚Äú. Normal distribution is also a stable distribution with an alpha = 2            This being so, - All stable distributions are infinitely divisible. - With the exception of the normal distribution (Œ± = 2), stable distributions are leptokurtotic and heavy-tailed distributions.            The characteristic function œÜ(t) of any probability distribution is just the Fourier transform of its probability density function f(x).            The density function (pdf) is therefore the inverse Fourier transform or the fast fourier of the characteristic function.       Although the probability density function for a general stable distribution cannot be written analytically, the general characteristic function can be expressed analytically.   Having understoodm these ten simple facts. You already have a mind map of the whole paper.   Instead of using normal use levy, but since levy does not have pdf, use fast fourier to find it. Fit the residuals of the market using the levy. Find the shape and scale parameters. Also the alpha and beta.    Figure 1.2 Dow Jones Levy Alpha Distribution   The files     DJIA - Dow Jones   PF - Pfizer   fft - Fast Fourier Transform djia.rmd is an R file making analysis on the Down Jones index for stated period. pfe.rmd - analysis on pfizer stock for stated period. fft.py - is the fast fourier transform implemented. levystable.py -  is the levy stable distribution inside scipy library. hs.csv is a combined data file having side by side stocks of dow jones and pfizer. pfe.csv has only pfizer historical prices    Figure 1.3 Predictions made using regular BS model.    Figure 1.4 Predictions made using levy alpha.   ","categories": [],
        "tags": [],
        "url": "/Option-Pricing/",
        "teaser": null
      },{
        "title": "Monte Carlo Methods 2",
        "excerpt":"Now, lets talk about something else apart from the binomial distribution. Say you have a circle, and you want to drop balls over it. One by one you drop a ball over the circle, say dish. The question is how many balls are gonna go inside it.   As a mathematician, anything that involves circles or curves or anything circul-ish should bring to your mind the symbol \\(\\pi\\). PI is a lot of things but just understanding that somehow a circle would be involved makes the whole thing wrapped up. Next the field around the circle most cases is usually a square. Look at the diagram below.   Notice how the balls might fall out of the circle and into that area. Would it be right if i said, the proability of all balls being in the circle is \\(\\pi/4\\)   Well, we will see. This is a mathematical standard but a simulation might better prove this. Hope your RStudio is open.   #Set the number of runs runs &lt;- 5000  #drop one ball using the function drop_ball drop_ball &lt;- function(){     #simulate the x-coordinate of the ball     x &lt;- runif(n = 1, min = 1e-12, max = .9999999999)        #simulate the y-coordinate of the ball     y &lt;- runif(n = 1, min = 1e-12, max = .9999999999)          #We are using the uniform distribution between 0 and 1      # It is a mathematical constant that anything that fulfills the rule below is a coordinate in the unit circle.     # return this as the answer, is it true or not     (x**2 + y**2) &lt; 1 }  mc &lt;- sum(replicate(runs, drop_ball))/runs  #print the proportion print(mc)   The answer printed above should fulfil the earlier rule that i mentioned of the Pr(a) coverging towards pi/4.   Monte Carlo has endless uses but a big part about simulations is knowing what you need to achieve. This will guide you on the right path and actions. It can be used for more versatile uses: predicting risk of a volatile security or stock, predicting number of items that will go to SCRAP in a conveyor chain and so many. Particle physics uses monte carlo a lot, i mean they dont have unlimited supply of particles üò¨.   This about sums up the Monte Carlo series. However for any queries, am a comet away. (Get it, coz comment is like cometü•≤. Bet you didnt see that coming, ditchya)  ","categories": [],
        "tags": [],
        "url": "/Monte-Carlo-Methods-2/",
        "teaser": null
      },{
        "title": "Monte Carlo Simulations",
        "excerpt":"What are Monte Carlo Simulation?   I know, I know, it looks like a guy‚Äôs name. Well nope it isn‚Äôt. Monte Carlo is a place. I always thought it Montreal. I wasn‚Äôt so wrong. They kinda both start with M, and, hear me out, the both in Canada. Monte Carlo is a town known for its many casinos. So many, so so many and what is it about about casinos, there is a lot of betting. Now betting is super random. I mean, yes, there are odds, but they remain odds, they could blantantly go the other way. But something interesting is that even with the most random of stuff in our universe, there is still some sort of pattern. This pattern however, is hard to notice with a single or even few number of tries.   Monte Carlo solves or comes close to solve this. Things we initially thought were random, turn out to have some sort of pattern due to the many times we will do the same task repetitively. This is called simulation and since we use monte carlo methods, its called monte carlo simulations. MC sim for the cool math kids.   Even though the method was discovered long ago, the computational capabilities of the Pentium üòÇ and earlier computers did not allow for such iterations. However, the M1, can do it in a flash. So if your are using Pentium, you need a makeover.   Lets do some examples:   1. Coin Tossing   I know its controversial why this would be a good example, since it has no real life use but trust me it makes the whole demonstration easy. So a coin toss is a bi thing. Not the LGBT kind, but a two-way thing, still not LGBT. You either get heads or tails, two way. As such we call it binomial. Each toss is also a single event, it literarily does not affect the next toss. These are discrete events.   Assume we do ten tosses, what would be the probability of getting 3 or more heads. So we need to do the ten tosses and add up all the number of times it was head. Say we have 7 times, our proability is 7/10 or 70% or 0.7. Depending on where you stand.   Ten seems little however, why not repeat the tosses, maybe a fairly large amount of times, N, say 10000 times, will we get the same probability. R makes this super simple   #Binomial Distribution # First set the number of runs you would like runs &lt;- 10000  ## One step will do a single round of toss 10 coins  and returns sum of the number of times the heads is &gt; 3  one_step &lt;- function(){   sum(sample(c(0,1),10,replace = T)) &gt; 3 } #For Monte Carlo, we just replicate the same process, the total number of times as initiated. Note the replicate function. # now we repeat that thing in 'runs' times. montecarlo &lt;- replicate(runs,one_step()) #this will print a lot of stuff but just note its trial you do not have to print(montecarlo)  #to find the probability just sum all times it was true and divide by total runs prob_toss &lt;- sum(montecarlo)/runs  print(prob_toss)   Well, this about sums it up for the first part of Monte Carlo, for the next section we will do an example with another distribution. This one will involve continous data so make sure yopu read it. If you liked, the read, give it a comment. Thank you.  ","categories": [],
        "tags": [],
        "url": "/Monte-Carlo-Simulations/",
        "teaser": null
      },{
        "title": "Convex Optimization",
        "excerpt":"The best solutions for each scenario. How better can we improve a system. One way of tackling this is through simulation.  ","categories": [],
        "tags": [],
        "url": "/Convex-Optimization/",
        "teaser": null
      },{
        "title": "Decision Trees",
        "excerpt":"#Importing dependancies import pandas as pd import numpy as np from sklearn.tree import DecisionTreeRegressor from sklearn.tree import DecisionTreeClassifier import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score   #Import data stored  train_data = pd.read_csv(\"train-data.csv\") test_data = pd.read_csv(\"test-data.csv\")  train_data.head() test_data.head()                                       Unnamed: 0       Name       Location       Year       Kilometers_Driven       Fuel_Type       Transmission       Owner_Type       Mileage       Engine       Power       Seats       New_Price                       0       0       Maruti Alto K10 LXI CNG       Delhi       2014       40929       CNG       Manual       First       32.26 km/kg       998 CC       58.2 bhp       4.0       NaN                 1       1       Maruti Alto 800 2016-2019 LXI       Coimbatore       2013       54493       Petrol       Manual       Second       24.7 kmpl       796 CC       47.3 bhp       5.0       NaN                 2       2       Toyota Innova Crysta Touring Sport 2.4 MT       Mumbai       2017       34000       Diesel       Manual       First       13.68 kmpl       2393 CC       147.8 bhp       7.0       25.27 Lakh                 3       3       Toyota Etios Liva GD       Hyderabad       2012       139000       Diesel       Manual       First       23.59 kmpl       1364 CC       null bhp       5.0       NaN                 4       4       Hyundai i20 Magna       Mumbai       2014       29000       Petrol       Manual       First       18.5 kmpl       1197 CC       82.85 bhp       5.0       NaN                     &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\"&gt;             &lt;/svg&gt;                             #preprocess the data and clean it class Preprocess():     def __init__(self, dataset):         self.dataset = dataset         return None      #This function drops unnecessary columns     def dropcolumns(self):         to_drop = [\"Unnamed: 0\", \"Name\", \"Location\", \"Kilometers_Driven\", \"Fuel_Type\", \"Transmission\", \"Owner_Type\", \"Power\", \"Seats\", \"New_Price\"]         return self.dataset.drop(             to_drop, inplace=True, axis=1         )      #This function drops all rows with null values from the dataset     def dropna(self):         col_drop = self.dropcolumns()         dropped = self.dataset.dropna()         return dropped      #This function cleans the units from the string columns     #It also concerts the clean columns which are in string format to numeric     def removeString(self):         self.dataset['Engine'] = self.dataset['Engine'].str.replace(r'\\D', '')         self.dataset['Engine'] = pd.to_numeric(self.dataset['Engine'])         self.dataset['Mileage'] = self.dataset['Mileage'].str.replace(r'\\D', '')         self.dataset['Mileage'] = pd.to_numeric(self.dataset['Mileage'])         return self.dataset      #This function calls the rest of the class     def clean(self):       self.removeString()       return self.dropna()    #clean the training and test data clean_train_data = Preprocess(train_data).clean()  clean_test_data = Preprocess(test_data).clean()  #show the clean data clean_train_data.head()   /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: FutureWarning: The default value of regex will change from True to False in a future version. /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: FutureWarning: The default value of regex will change from True to False in a future version.                                       Year       Mileage       Engine       Price                       0       2010       266.0       998.0       1.75                 1       2015       1967.0       1582.0       12.50                 2       2011       182.0       1199.0       4.50                 3       2012       2077.0       1248.0       6.00                 4       2013       152.0       1968.0       17.74                     &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\"&gt;             &lt;/svg&gt;                             #split the data to features(X) and targets(labels)(Y) class splitXY():     def __init__(self, dataset, label):         self.dataset = dataset         self.label = label         return None     #this function creates the features and labels     def splitlabel(self):         X = self.dataset.drop(self.label, axis = 1).values         y = self.dataset[self.label[0]].values          return X, y       #splot the data to train and test data     def splitdata(self):         X, y = self.splitlabel()          X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, random_state=25)          # print(f\"No. of training examples: {training_data.shape[0]}\")         # print(f\"No. of testing examples: {testing_data.shape[0]}\")         return X_train, X_test, y_train, y_test   #First Approach: Using the Target Variable Price as a continous variable and thus regression by decision trees   X_train, X_test, y_train, y_test = splitXY(clean_train_data, [\"Price\"]).splitdata() #Show the training data X_train, y_train   (array([[2013.,  284., 1248.],         [2015.,  182., 1248.],         [2015., 1757., 1193.],         ...,         [2005.,  110., 2987.],         [2018., 1602., 1373.],         [2016., 2014., 1498.]]),  array([ 4.95,  4.3 ,  4.52, ..., 10.  ,  8.25,  6.3 ]))   #Instanciate the decision tree regressors fit_1 = DecisionTreeRegressor(max_depth=2) fit_2 = DecisionTreeRegressor(max_depth=5)   #Fit the data to the instanciated model fit_1.fit(X_train, y_train) fit_2.fit(X_train, y_train)   DecisionTreeRegressor(max_depth=5)   fit_2.score(X_train, y_train)   0.7678476226395207   fit_2.get_n_leaves()   32   cross_val_score(fit_2, X_train, y_train, cv=10)   array([0.81216411, 0.76445921, 0.70394034, 0.66493376, 0.64491868,        0.71417788, 0.61902476, 0.7185033 , 0.72539609, 0.68439817])   from math import sqrt #Make predictions of the model using the test dataset #X_test = clean_test_data y_1 = fit_1.predict(X_test) y_2 = fit_2.predict(X_test)   #Calculate sum of squared errors err = y_test - y_2 print((sum(err**2)))   30397.07422407224   #Second Approach: Make classes/bins using Target Variable Price and thus classifcation using decision trees   #create three classes of cheap, middle and expensive clean_train_data['Label'] = pd.cut(x = clean_train_data['Price'], bins = [0, 4, 7, 15, 40, 200], labels=['Cheap', 'Low-Mid', 'Mid-High','Expensive', \"Super-Expensive\"]) clean_train_data['Label'].value_counts()   /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning:  A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead  See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy         Cheap              1951 Low-Mid            1793 Mid-High           1258 Expensive           809 Super-Expensive     170 Name: Label, dtype: int64   X_train, X_test, y_train, y_test = splitXY(clean_train_data, [\"Label\",\"Price\"]).splitdata() #Show the training data X_train, y_train   (array([[2013.,  284., 1248.],         [2015.,  182., 1248.],         [2015., 1757., 1193.],         ...,         [2005.,  110., 2987.],         [2018., 1602., 1373.],         [2016., 2014., 1498.]]),  ['Low-Mid', 'Low-Mid', 'Low-Mid', 'Low-Mid', 'Low-Mid', ..., 'Cheap', 'Low-Mid', 'Mid-High', 'Mid-High', 'Low-Mid']  Length: 4784  Categories (5, object): ['Cheap' &lt; 'Low-Mid' &lt; 'Mid-High' &lt; 'Expensive' &lt; 'Super-Expensive'])   clf = DecisionTreeClassifier(random_state = 34)# max_depth = 5) clf.fit(X_train, y_train)   DecisionTreeClassifier(random_state=34)   cross_val_score(clf, X_train, y_train , cv=10)   array([0.77244259, 0.76617954, 0.76617954, 0.76200418, 0.80125523,        0.75313808, 0.76569038, 0.78870293, 0.77824268, 0.76987448])   clf.score(X_test, y_test)   0.772765246449457   clf.predict(clean_test_data)   /usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names   f\"X has feature names, but {self.__class__.__name__} was fitted without\"      array(['Cheap', 'Cheap', 'Expensive', ..., 'Cheap', 'Cheap', 'Expensive'],       dtype=object)   ","categories": [],
        "tags": [],
        "url": "/Decision-Trees/",
        "teaser": null
      }]
