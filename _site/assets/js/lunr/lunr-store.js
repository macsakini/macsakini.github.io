var store = [{
        "title": "Algorithms Insertion Sort",
        "excerpt":"Algorithms - Insertion Sort  import java.util.Arrays;  class insertionsort {     static int[] value = {14,33,27,35,10};     public static void main(String[] args){         int holePosition;         int valueToInsert;          for(int i = 1; i &lt; value.length; i++ ){             valueToInsert = value[i];                          holePosition = i;              while(holePosition &gt; 0 &amp;&amp; value[holePosition - 1] &gt; valueToInsert){                                  value[holePosition] = value[holePosition - 1];                                  holePosition = holePosition - 1;                  System.out.println(Arrays.toString(value));                              }              value[holePosition] = valueToInsert;         }          System.out.println(Arrays.toString(value));     }     }  ","categories": [],
        "tags": [],
        "url": "/Algorithms-Insertion-Sort/",
        "teaser": null
      },{
        "title": "Algorithms Merge Sort",
        "excerpt":"Algorithms - Merge Sort  import java.util.Arrays;  class mergesort {     static int[] value = {14,33,27,19,42,44,78,6};              static int n = value.length;     public static void main(String[] args){         mergesort(value, n);     }      public static void mergesort(int[] arr, int n){         if(n == 1){             return;         }         // System.out.println((n));          // System.out.println(Arrays.toString(arr));          int mid  = n/2;          int[] l1 = new int[mid];                  int[] l2 = new int[n - mid];                   for (int i = 0; i &lt; mid; i++) {             l1[i] = arr[i];         }          for (int i = mid; i &lt; n; i++) {             l2[i - mid] = arr[i];         }          // System.out.println(Arrays.toString(l1));         // System.out.println(Arrays.toString(l2));          mergesort(l1, mid);          mergesort(l2, n-mid);          merge(l1,l2);       }      public static void merge(int[] a, int[] b){         int[] c = new int[0];          System.out.println(a.length);                   while(a.length &gt; 0 &amp;&amp; b.length &gt; 0){             if(a[0] &gt; b[0]){                 c[c.length] = (b[0]);                 Arrays.asList(b).remove(b[0]);             }else{                 c[c.length] = a[0];                 Arrays.asList(a).remove(a[0]);             }         }          while(a.length &gt; 0 ){             c[c.length] = a[0];             Arrays.asList(a).remove(a[0]);         }          while(b.length &gt; 0 ){             c[c.length] = (b[0]);             Arrays.asList(b).remove(b[0]);         }          System.out.println(Arrays.toString(c));      }      }  ","categories": [],
        "tags": [],
        "url": "/Algorithms-Merge-Sort/",
        "teaser": null
      },{
        "title": "Algorithms Selection Sort",
        "excerpt":"Algorithms - Select Sort   import java.util.Arrays;  class selectsort {     static int[] value = {14,33,27,245,10,35,19,42,44,78,6};     public static void main(String[] args){         int n = value.length;          System.out.println(Arrays.toString(value));                  for(int i = 0; i &lt; (n - 1); i++){             int min = i;              for(int j = (i + 1); j &lt; n; j++){                 System.out.println(Arrays.toString(value));                  if (value[j] &lt; value[min]) {                     min = j;                 }             }             System.out.println(Arrays.toString(value));              if (min != i){                 swap(min, i);             }         }         System.out.println(Arrays.toString(value));      }         public static String swap(int i, int j){         int temp = value[i];         value[i] = value[j];         value[j] = temp;         return\"success\";     } }  ","categories": [],
        "tags": [],
        "url": "/Algorithms-Selection-Sort/",
        "teaser": null
      },{
        "title": "Algorithms Bubble Sort",
        "excerpt":"Algorithms - Bubble Sort  import java.util.Arrays;  class BubbleSort{     //static ArrayList&lt;Integer&gt; value = new ArrayList&lt;Integer&gt;(Arrays.asList(14,33,27,35,10));     static int[] value = {14,33,27,35,10};          public static void main(String[] args){          System.out.println(Arrays.toString(value));         for(int i = 0; i &lt; (value.length - 1); i++){             Boolean swapped = false;             for(int j = 0; j &lt; (value.length - 1); j++){                 System.out.println(j);                 if(value[j] &gt; value[j+1]){                     swap(j, j+1);                     swapped = true;                     System.out.println(Arrays.toString(value));                 }             }             System.out.println(swapped);             if(swapped == false){                 break;             }         }          System.out.println(Arrays.toString(value));     }      public static String swap(int i, int j){         int temp = value[i];         value[i] = value[j];         value[j] = temp;         return\"success\";     } }  ","categories": [],
        "tags": [],
        "url": "/Algorithms-Bubble-Sort/",
        "teaser": null
      },{
        "title": "Ds Stacks And Queues",
        "excerpt":"Data Structures - Stacks and Queues in Java.   Stacks  You are probably wondering, what are stacks. Well Stacks are stacks 😛. It is in the name right. Well a stack is how you put your plates. Or how you play that cup game. You just put things over each other.   The main principle behind stacks is LIFO: Last In First Out.  The main two actions a stack performs is pushing and poping. You push something into the stack and you pop something out of the stack.   Java has a built in stack framework. Find below an image showcasing this characteristic.   Image Goes HERE….   Java implements it quite nicely, create a new stack called games. To add items into the games stack just use the add function. To remove stuff out of the games stack just use pop.   The second example shows a character stack list. Here am justing adding single characters such as ‘A’, ‘B’ and so on.  import java.util.Stack;  public class stacky {     public static void main(String[] args){         Stack&lt;String&gt; games = new Stack&lt;String&gt;();          games. add(\"Call of Duty\");          games.add(\"Super Monkey Ball\");          games.add(\"Guitar Hero\");          System.out.println(games);          //if you would like to get topmost one          System.out.println(games.pop());          //to get second from top         System.out.println(games.pop());          // to get remaining stack after two pop operations         System.out.println(games);          //to check the first item on the stack          System.out.println(games.peek());          // Y         // B         // R          Stack&lt;Character&gt; tower = new Stack&lt;Character&gt;();          tower.add('R');         tower.add('B');         tower. add('R');          System.out.println(tower.empty());         System.out.println(tower.get(1));         System.out.println(tower.set(1, 'P'));         System.out.println(tower.size());        } }   The queue is just stacks wacky brother. Same concept except the queue is an actual groceries shop queue or the line at the hospital. So whoever walked in first, gets served first. This is an interesting principle called FIFO: First In First Out.   The actions that describe a queue at best are enqueue and dequeue. However, Java makes it simple, you add or poll a queue. Make sure to copy paste the code and try it out.   Image does here   Queues  import java.util.LinkedList; import java.util.Queue;  public class queueish {     public static void main(String[] args){         //Example one         Queue&lt;String&gt; bbq = new LinkedList&lt;String&gt;();          bbq.add(\"Jeff\");          bbq.add(\"Tyrique\");          bbq.add(\"Susan\");                  bbq.peek();          System.err.println(bbq);          //Jeff         bbq.poll();         //Tyrqique         bbq.poll();         //Susan         System.out.println(bbq.poll());          //Example two          Queue&lt;String&gt; q = new LinkedList&lt;String&gt;();          q.add(\"A\");         q.add(\"B\");         q.add(\"C\");          q.poll();          System.out.println(q);          System.out.println(q.size());          System.out.println(q.contains(\"A\"));          System.out.println(q.toArray()[1]);      }  }  ","categories": [],
        "tags": [],
        "url": "/DS-Stacks-and-Queues/",
        "teaser": null
      },{
        "title": "Ds Linked Lists Arrays",
        "excerpt":"Data Structures - Linked Lists, Arrays, Hashmap   This is a collection of some of the most fundamental data structures and functions done in Java.   Each file in this repository is a snippet of the java function assigned. The name of the repository is also closely related to the task at hand. i.e getset.java concernes getters and setters.   Linked Lists  import java.util.Iterator; import java.util.LinkedList;  class Linkylist {     public static void main(String[] args){         LinkedList&lt;String&gt; linky = new LinkedList&lt;String&gt;();          //you can add elements to the LinkedAList          linky.add(\"Rob\");         linky.add(\"animal\");         linky.add(\"Hose\");          linky.getFirst();         //linky.remove();          System.out.println(linky);          LinkedList&lt;Integer&gt; numlinky = new LinkedList&lt;Integer&gt;();          numlinky.add(1);         numlinky.add(78);         numlinky.add(456);          System.out.println(numlinky.get(2));          //iterating through the linkedlist to find a number          Iterator it = numlinky.iterator();          while(it.hasNext()){             if((int) it.next() == 78){                 System.out.println(\"We have found 78\");             }         }               } }  Arrays   import java.util.ArrayList; import java.util.List;  class arrayish {     public static void main(String[] args){         List&lt;Integer&gt; a = new ArrayList&lt;Integer&gt;();          a.add(10);         a.add(20);          List &lt;String&gt; b = new ArrayList&lt;String&gt;();          for(int i : a){             System.out.println(String.valueOf(i));             b.add(String.valueOf(i));         }          System.out.println(b);       } }  HashMaps and HashTables   import java.util.HashMap;  public class hashmapy {     public static void main(String[] args){         int a, b, c;         a = 1;         b = 2;         c = 3;          HashMap&lt;String,Integer&gt; happy = new HashMap&lt;&gt;();          happy.put(\"a\", a);         happy.put(\"b\", b);         happy.put(\"c\",c);           System.out.println(happy.get(\"c\"));     })  }   ","categories": [],
        "tags": [],
        "url": "/DS-Linked-Lists-Arrays/",
        "teaser": null
      },{
        "title": "Linear Algebra Series 2",
        "excerpt":"Linear Algebra Series 2 - Linear Independence and Dependence   I will try to avoid math, so test 1,   \\[A \\implies B \\, \\exists \\, x \\: \\in \\R\\]  And yes, it is working. This was just a test, dont copy that. Now lets get down to it.  First, order of business, is linear independence. This is a major concept in linear algebra but one the Brits seem to never understand 🤨. I’ll describe both the formal and you versions for better understanding.   Formal Definition.   Linear independence. Say we have set of vectors \\(x_1, x_2 \\land x_3\\) in a linear subspace, they can only be termed independent if \\(\\exists \\, c_1, c_2, c_3\\) such that:   \\[c_1 * x_1 + c_2 * x_2 + c_3 * x_3 = 0\\]  \\(\\land \\: \\therefore\\) a null vector, where \\(c_1 = c_2 = c_3 = 0\\)   Linear dependence on the other hand is the same except atleast one scalar is non-zero.   Note Linearly dependent vectors can be written as a linear combination of linearly independent vectors.   Logical Definition.  ","categories": [],
        "tags": [],
        "url": "/Linear-Algebra-Series-2/",
        "teaser": null
      },{
        "title": "Modality And  seasonality On Security Prices",
        "excerpt":"Modality and Seasonality on Security Prices in the DOW   The data in the data folder is temporal and thus the models to be used are time series models. The analysis is not geared towards prediction but is focused on finding the right parameters and metrics that best decsribes these datasets. It is also geared by finding which of the time series approaches used in project is more powerful, compact and accurate compared to the other.    Figure 1.1 The daily returns of the security.   The following is a snippet of the exponential smoothing done in R language:   Exponential smoothing.     Exponential smoothing. This method works by averaging historical data and using those weights to create a more robust and noise-less trend compared to actual real-time data with high critical volatility. The methodology creates a safe space for predicting future values in a certain trend space. It reduces the effect of the trend on the movement and tries to predict future values.    Figure 1.2 Fitted ES model.   #Exponential smoothing model exponential &lt;- function(data){   summary(es(data,h=100, holdout=FALSE, silent=FALSE, alpha=.2))   } exponential(data_x)   ARIMA.     ARIMA. This is a model that is used in finding the flow of data using a regression-like model that is based on previous time stamps. It is a combination of AR or autoregressive and MA moving average models with a integration using differencing to make the non-stationary data stationary. It will be used to show, seasonality, trend and autocorrelation.  Figure 1.3 Arime model fitted on the data.   The following is a snippet of the ARIMA code done in the R language:  #AUTOMATIC ARIMA autoarima &lt;- function(data){   x &lt;- auto.arima(data)   print(checkresiduals(x))   print(summary(x))   autoplot(x) } autoarima(data_x)  #ARIMA MODEL OF CHOICE setarima &lt;- function(data){   x &lt;- Arima(data, order=c(1,1,1))   print(summary(x))   print(checkresiduals(x))   autoplot(x) } setarima(data_x)   Regression     Regression. Regression done on a time series perspective has to be dynamic and will show the level of correlation. Regular regression is done using the concerned variable to a relatable variable in same data space. In our scenario since these are parallel time-series, regression can be done directly to the trans-mutated date-time variable or it can be done against the other time-series and the greatest correlation of these can be determined using the coefficients.   A lot of the data gathered everyday is __time series data__. From individuals, to organizations there has been a fourth industrial revolution of data. Analytics is becoming part of everyday life and slowly by slowly it is being integrated into everyday life. From insurance using telematics to fitness using smart watches, these are just some of the applications that use data and data analysis.  ","categories": [],
        "tags": [],
        "url": "/Modality-and-Seasonality-on-Security-Prices/",
        "teaser": null
      },{
        "title": "Linear Algebra Series",
        "excerpt":"Linear Algebra Series 1 - Introduction   This particular blog post will cover at large the basics of linear algebra and how to understand it in thirty minutes. Cap. I would give me a year😬.   We are gonna use a lot of Python for the computations so be ready to do some nice copy paste. The first section involves importing all necessary libraries into the workspace. In this case they are only two:🥲. Our star library for today is the SciPy library which will mostly deal with scientific calculations and as such scientific subjects, disciples, I mean disciplines, notations, et al….   import numpy as np import scipy.linalg as sp  Follow by, creating a matrix using the array function by numpy. The formula remains standard 😊 RC which is Roman Catholic which means Rows then Columns   a = np.array([[1,2,3],[4,5,6],[7,88,9]]) b = np.array([[1,2,3],[4,5,6],[7,8,9]])  Basic Operations on Matrices   Finding the determinant. NOTE:= The determinant is a scalar value.  Say we have a matrix A = [[a,b],[c,d]]: We find the detriminant using the formula \\(Det(A) = (a*d)-(c*b)\\). The determinant and this is not a formal definition, can be thought of as the “volume” of the space occupied by the matrix.   sp.det(a)  If the determinant is zero, this matrix becomes a singular matrix. Otherwise, it is just a regular square matrix. I say square as there exist other types of Matrices, but these will be discussed in other sections of the series.   B is an example singular matrix. Ensure to test with B as well, in place of A, just to get a feel of the concept.                  Next, we can find the norm of the matrix :=       A       . Similar to any vector, matrices have a norm which is just their magnitude (size).           The norm of the matrix can be derived using the norm function.   sp.norm(a)   The inverse of the matrix. As the word implies, is the alternative matrix in the opposite, say what, “direction” but orthogonally. Putting it in another way, interchange a and d, give b and c, negative signs and “vwaalah” inverse. [[a,b],[c,d]] \\ \\Longrightarrow \\ [[d,-b], [-c,a]]. However, a more formal way of doing it is using the inv function provided by SciPy. Mathematicians, use a matrix of signs to find the inverse for bigger matrices. 3 x 3, 4 x 4 and the gang.   Note a singular matrix, does not have an inverse.  try:   print(sp.inv(a)) except:   print(\"This matrix is a singular matrix\")   Finding the Eigenvalues and Eigenvectors  This section just shows the way of finding the eigen’s using SciPy. However, I will add a link down below to an exclusive post, fully focused, on what these are, how the help and where to use them. They carry a major role in CS, Physics, Chemistry… as such they are very important.   Finding the eigenvalues and eigenvectors.  sp.eig(a)  This specifically prints out the eigenvalues.  sp.eigvals(a)  Lets talk about matrix decomposition  There are so many ways of decomposing a matrix. Name a few, LU, QR, QZ, Polar, SVD and so much more. You are probably wondering, who are all these Chinese babies. However, i will disappoint you, these are decomposition styles. I will cover only the most important among these.   They all try to achieve the same task: make matrix computations easier. Think of decomposition as making matrices into legos. The matrix doesn’t loose its end value, we just break it into pieces, that when we remap together reform the broken down matrix. This happens so that they can be used in Ninjago and be used differently and more efficiently at various tasks. Imagine, 9 is a matrix, if I break it into 4 and 5 or even 2, 4 and 3, we havent really changed the 9. I have just broken it down or decomposed it into a number of values. Decomposition is somewhat similar, not direct, yes, but somewhat similar.   LU Decomposition  This is a pivoted LU decomposition.  lu = sp.lu(a)  print(lu[0], \"\\n\") print(lu[1], \"\\n\") print(lu[2], \"\\n\")   sp.lu_factor(a)  Singular Value Decomposition  This is an SVD.  svd = sp.svd(a)  print(svd[0], \"\\n\") print(svd[1], \"\\n\") print(svd[2], \"\\n\")  sp.svdvals(a)  Find the orthonormal basis for A using SVD  Basis and orthonormal shall be covered in the next post. Be sure to head there.  sp.orth(a)  Finds the orthonormal basis for the null_space using SVD  sp.null_space(a)  Polar Decomposition  sp.polar(a)  Matrix can also be considered as regular numbers with regular applications.  This definitively means, you can carry out operations similar to those on real and natural numbers without any siginificant changes. A number of the operations include:      Finding the exponential of the matrix.     sp.expm(a)           Finding the sine of the matrix.     sp.sinm(a)           Finding the cosine.     sp.cosm(a)           Finding the tangent of the matrix.     sp.tanm(a)           Finding the square root of the matrix.     sp.sqrtm(a)           And even applying a function to the matrix. For example, this is the dot product of the matrix. ```python print(sp.funm(a, lambda x: x*x))   print(“\\n”)   print(np.dot(a,a))   ## There is a whole lot of special matrices. However i will just build a few to demonstrate.  This creates a 3 by 3 hilbert matrix. Hilbert spaces have special characteristics coupled up with the Inner Product and the Banach Space and so on... but i will also discuss this in later sections and in full length. I mean, this is the introductory, you wouldn't want me on Banach Spaces, right. ```python sp.hilbert(3)  Hadamard matrix is also a special matrix. Some arab guy made it. Its mostly used in quantum computations due to its nature of just being 1’s and -1’s, big deal, whoop. ITS A VERY BIG DEAL.🫡 .They are used for the opening and closing of logic gates in quantum computers coupled with superposition which i will not even dare mention.  sp.hadamard(4)   This creates a discrete fourier transform matrix. FO-RII-EEEH.  sp.dft(3)   This creates a helmert matrix. Its kind of used with the hadamard, but do some research.  sp.helmert(3)    That mostly sums it up. This is an introductory session on using the scipy library to evaluate some of Linear Algebra’s most common functions. In the next, sections, I will get in deeper into the aforementioned topics, be there. Stay Tuned.😌  ","categories": [],
        "tags": [],
        "url": "/Linear-Algebra-Series/",
        "teaser": null
      },{
        "title": "Option Pricing",
        "excerpt":"Option Analysis and Prediction using Fast Fourier Transform and Levy Alpha Stable Distributions   This project uses a number of concepts.           Black Scholes model.            Capital Assets Pricing Model (CAPM).            Levy Alpha Stable Distributions.            Market Model.            Fourier and Inverse Fourier Transformations.       The files of the project are all over the place but each file deals with a certain topic. I have also included a docx file explaining the entire theorem in the data folder.   However the following concept can be critical to understand.          Options are regularly proced using the BS model.            Volatility in stock markets and securities are regularly functions of normal distributions according to a number of models.        Figure 1.1 CAPM Model           Technical analysis incorporates normal distributions. This can also be seen in the thrid graph in the image above. The residuals are trying to get fit under a normal curve.            Beta Coefficient explains risk in a regression model of market prices with underlying asset.            Beta distributions can take different forms depending on the shape and scale parameters assigned to it. Bet coefficients are not related to beta distributions            Prior analysis has shown that in some cases fitting a different distribution similar to the normal explains better the fractals and risk numbers in a market.            Levy Distributions are extracts of the stable distributions. According to wikipedia, beta distributions are not \"”not analytically expressible, except for some parameter values”“. Normal distribution is also a stable distribution with an alpha = 2            This being so, - All stable distributions are infinitely divisible. - With the exception of the normal distribution (α = 2), stable distributions are leptokurtotic and heavy-tailed distributions.            The characteristic function φ(t) of any probability distribution is just the Fourier transform of its probability density function f(x).            The density function (pdf) is therefore the inverse Fourier transform or the fast fourier of the characteristic function.       Although the probability density function for a general stable distribution cannot be written analytically, the general characteristic function can be expressed analytically.   Having understoodm these ten simple facts. You already have a mind map of the whole paper.   Instead of using normal use levy, but since levy does not have pdf, use fast fourier to find it. Fit the residuals of the market using the levy. Find the shape and scale parameters. Also the alpha and beta.    Figure 1.2 Dow Jones Levy Alpha Distribution   The files     DJIA - Dow Jones   PF - Pfizer   fft - Fast Fourier Transform djia.rmd is an R file making analysis on the Down Jones index for stated period. pfe.rmd - analysis on pfizer stock for stated period. fft.py - is the fast fourier transform implemented. levystable.py -  is the levy stable distribution inside scipy library. hs.csv is a combined data file having side by side stocks of dow jones and pfizer. pfe.csv has only pfizer historical prices    Figure 1.3 Predictions made using regular BS model.    Figure 1.4 Predictions made using levy alpha.   ","categories": [],
        "tags": [],
        "url": "/Option-Pricing/",
        "teaser": null
      },{
        "title": "Monte Carlo Methods 2",
        "excerpt":"Now, lets talk about something else apart from the binomial distribution. Say you have a circle, and you want to drop balls over it. One by one you drop a ball over the circle, say dish. The question is how many balls are gonna go inside it.   As a mathematician, anything that involves circles or curves or anything circul-ish should bring to your mind the symbol \\(\\pi\\). PI is a lot of things but just understanding that somehow a circle would be involved makes the whole thing wrapped up. Next the field around the circle most cases is usually a square. Look at the diagram below.   Notice how the balls might fall out of the circle and into that area. Would it be right if i said, the proability of all balls being in the circle is \\(\\pi/4\\)   Well, we will see. This is a mathematical standard but a simulation might better prove this. Hope your RStudio is open.   #Set the number of runs runs &lt;- 5000  #drop one ball using the function drop_ball drop_ball &lt;- function(){     #simulate the x-coordinate of the ball     x &lt;- runif(n = 1, min = 1e-12, max = .9999999999)        #simulate the y-coordinate of the ball     y &lt;- runif(n = 1, min = 1e-12, max = .9999999999)          #We are using the uniform distribution between 0 and 1      # It is a mathematical constant that anything that fulfills the rule below is a coordinate in the unit circle.     # return this as the answer, is it true or not     (x**2 + y**2) &lt; 1 }  mc &lt;- sum(replicate(runs, drop_ball))/runs  #print the proportion print(mc)   The answer printed above should fulfil the earlier rule that i mentioned of the Pr(a) coverging towards pi/4.   Monte Carlo has endless uses but a big part about simulations is knowing what you need to achieve. This will guide you on the right path and actions. It can be used for more versatile uses: predicting risk of a volatile security or stock, predicting number of items that will go to SCRAP in a conveyor chain and so many. Particle physics uses monte carlo a lot, i mean they dont have unlimited supply of particles 😬.   This about sums up the Monte Carlo series. However for any queries, am a comet away. (Get it, coz comment is like comet🥲. Bet you didnt see that coming, ditchya)  ","categories": [],
        "tags": [],
        "url": "/Monte-Carlo-Methods-2/",
        "teaser": null
      },{
        "title": "Monte Carlo Simulations",
        "excerpt":"What are Monte Carlo Simulation?   I know, I know, it looks like a guy’s name. Well nope it isn’t. Monte Carlo is a place. I always thought it Montreal. I wasn’t so wrong. They kinda both start with M, and, hear me out, the both in Canada. Monte Carlo is a town known for its many casinos. So many, so so many and what is it about about casinos, there is a lot of betting. Now betting is super random. I mean, yes, there are odds, but they remain odds, they could blantantly go the other way. But something interesting is that even with the most random of stuff in our universe, there is still some sort of pattern. This pattern however, is hard to notice with a single or even few number of tries.   Monte Carlo solves or comes close to solve this. Things we initially thought were random, turn out to have some sort of pattern due to the many times we will do the same task repetitively. This is called simulation and since we use monte carlo methods, its called monte carlo simulations. MC sim for the cool math kids.   Even though the method was discovered long ago, the computational capabilities of the Pentium 😂 and earlier computers did not allow for such iterations. However, the M1, can do it in a flash. So if your are using Pentium, you need a makeover.   Lets do some examples:   1. Coin Tossing   I know its controversial why this would be a good example, since it has no real life use but trust me it makes the whole demonstration easy. So a coin toss is a bi thing. Not the LGBT kind, but a two-way thing, still not LGBT. You either get heads or tails, two way. As such we call it binomial. Each toss is also a single event, it literarily does not affect the next toss. These are discrete events.   Assume we do ten tosses, what would be the probability of getting 3 or more heads. So we need to do the ten tosses and add up all the number of times it was head. Say we have 7 times, our proability is 7/10 or 70% or 0.7. Depending on where you stand.   Ten seems little however, why not repeat the tosses, maybe a fairly large amount of times, N, say 10000 times, will we get the same probability. R makes this super simple   #Binomial Distribution # First set the number of runs you would like runs &lt;- 10000  ## One step will do a single round of toss 10 coins  and returns sum of the number of times the heads is &gt; 3  one_step &lt;- function(){   sum(sample(c(0,1),10,replace = T)) &gt; 3 } #For Monte Carlo, we just replicate the same process, the total number of times as initiated. Note the replicate function. # now we repeat that thing in 'runs' times. montecarlo &lt;- replicate(runs,one_step()) #this will print a lot of stuff but just note its trial you do not have to print(montecarlo)  #to find the probability just sum all times it was true and divide by total runs prob_toss &lt;- sum(montecarlo)/runs  print(prob_toss)   Well, this about sums it up for the first part of Monte Carlo, for the next section we will do an example with another distribution. This one will involve continous data so make sure yopu read it. If you liked, the read, give it a comment. Thank you.  ","categories": [],
        "tags": [],
        "url": "/Monte-Carlo-Simulations/",
        "teaser": null
      },{
        "title": "Convex Optimization",
        "excerpt":"The best solutions for each scenario. How better can we improve a system. One way of tackling this is through simulation.  ","categories": [],
        "tags": [],
        "url": "/Convex-Optimization/",
        "teaser": null
      },{
        "title": "Decision Trees",
        "excerpt":"#Importing dependancies import pandas as pd import numpy as np from sklearn.tree import DecisionTreeRegressor from sklearn.tree import DecisionTreeClassifier import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score   #Import data stored  train_data = pd.read_csv(\"train-data.csv\") test_data = pd.read_csv(\"test-data.csv\")  train_data.head() test_data.head()                                       Unnamed: 0       Name       Location       Year       Kilometers_Driven       Fuel_Type       Transmission       Owner_Type       Mileage       Engine       Power       Seats       New_Price                       0       0       Maruti Alto K10 LXI CNG       Delhi       2014       40929       CNG       Manual       First       32.26 km/kg       998 CC       58.2 bhp       4.0       NaN                 1       1       Maruti Alto 800 2016-2019 LXI       Coimbatore       2013       54493       Petrol       Manual       Second       24.7 kmpl       796 CC       47.3 bhp       5.0       NaN                 2       2       Toyota Innova Crysta Touring Sport 2.4 MT       Mumbai       2017       34000       Diesel       Manual       First       13.68 kmpl       2393 CC       147.8 bhp       7.0       25.27 Lakh                 3       3       Toyota Etios Liva GD       Hyderabad       2012       139000       Diesel       Manual       First       23.59 kmpl       1364 CC       null bhp       5.0       NaN                 4       4       Hyundai i20 Magna       Mumbai       2014       29000       Petrol       Manual       First       18.5 kmpl       1197 CC       82.85 bhp       5.0       NaN                     &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\"&gt;             &lt;/svg&gt;                             #preprocess the data and clean it class Preprocess():     def __init__(self, dataset):         self.dataset = dataset         return None      #This function drops unnecessary columns     def dropcolumns(self):         to_drop = [\"Unnamed: 0\", \"Name\", \"Location\", \"Kilometers_Driven\", \"Fuel_Type\", \"Transmission\", \"Owner_Type\", \"Power\", \"Seats\", \"New_Price\"]         return self.dataset.drop(             to_drop, inplace=True, axis=1         )      #This function drops all rows with null values from the dataset     def dropna(self):         col_drop = self.dropcolumns()         dropped = self.dataset.dropna()         return dropped      #This function cleans the units from the string columns     #It also concerts the clean columns which are in string format to numeric     def removeString(self):         self.dataset['Engine'] = self.dataset['Engine'].str.replace(r'\\D', '')         self.dataset['Engine'] = pd.to_numeric(self.dataset['Engine'])         self.dataset['Mileage'] = self.dataset['Mileage'].str.replace(r'\\D', '')         self.dataset['Mileage'] = pd.to_numeric(self.dataset['Mileage'])         return self.dataset      #This function calls the rest of the class     def clean(self):       self.removeString()       return self.dropna()    #clean the training and test data clean_train_data = Preprocess(train_data).clean()  clean_test_data = Preprocess(test_data).clean()  #show the clean data clean_train_data.head()   /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: FutureWarning: The default value of regex will change from True to False in a future version. /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: FutureWarning: The default value of regex will change from True to False in a future version.                                       Year       Mileage       Engine       Price                       0       2010       266.0       998.0       1.75                 1       2015       1967.0       1582.0       12.50                 2       2011       182.0       1199.0       4.50                 3       2012       2077.0       1248.0       6.00                 4       2013       152.0       1968.0       17.74                     &lt;svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"        width=\"24px\"&gt;             &lt;/svg&gt;                             #split the data to features(X) and targets(labels)(Y) class splitXY():     def __init__(self, dataset, label):         self.dataset = dataset         self.label = label         return None     #this function creates the features and labels     def splitlabel(self):         X = self.dataset.drop(self.label, axis = 1).values         y = self.dataset[self.label[0]].values          return X, y       #splot the data to train and test data     def splitdata(self):         X, y = self.splitlabel()          X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, random_state=25)          # print(f\"No. of training examples: {training_data.shape[0]}\")         # print(f\"No. of testing examples: {testing_data.shape[0]}\")         return X_train, X_test, y_train, y_test   #First Approach: Using the Target Variable Price as a continous variable and thus regression by decision trees   X_train, X_test, y_train, y_test = splitXY(clean_train_data, [\"Price\"]).splitdata() #Show the training data X_train, y_train   (array([[2013.,  284., 1248.],         [2015.,  182., 1248.],         [2015., 1757., 1193.],         ...,         [2005.,  110., 2987.],         [2018., 1602., 1373.],         [2016., 2014., 1498.]]),  array([ 4.95,  4.3 ,  4.52, ..., 10.  ,  8.25,  6.3 ]))   #Instanciate the decision tree regressors fit_1 = DecisionTreeRegressor(max_depth=2) fit_2 = DecisionTreeRegressor(max_depth=5)   #Fit the data to the instanciated model fit_1.fit(X_train, y_train) fit_2.fit(X_train, y_train)   DecisionTreeRegressor(max_depth=5)   fit_2.score(X_train, y_train)   0.7678476226395207   fit_2.get_n_leaves()   32   cross_val_score(fit_2, X_train, y_train, cv=10)   array([0.81216411, 0.76445921, 0.70394034, 0.66493376, 0.64491868,        0.71417788, 0.61902476, 0.7185033 , 0.72539609, 0.68439817])   from math import sqrt #Make predictions of the model using the test dataset #X_test = clean_test_data y_1 = fit_1.predict(X_test) y_2 = fit_2.predict(X_test)   #Calculate sum of squared errors err = y_test - y_2 print((sum(err**2)))   30397.07422407224   #Second Approach: Make classes/bins using Target Variable Price and thus classifcation using decision trees   #create three classes of cheap, middle and expensive clean_train_data['Label'] = pd.cut(x = clean_train_data['Price'], bins = [0, 4, 7, 15, 40, 200], labels=['Cheap', 'Low-Mid', 'Mid-High','Expensive', \"Super-Expensive\"]) clean_train_data['Label'].value_counts()   /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning:  A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead  See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy         Cheap              1951 Low-Mid            1793 Mid-High           1258 Expensive           809 Super-Expensive     170 Name: Label, dtype: int64   X_train, X_test, y_train, y_test = splitXY(clean_train_data, [\"Label\",\"Price\"]).splitdata() #Show the training data X_train, y_train   (array([[2013.,  284., 1248.],         [2015.,  182., 1248.],         [2015., 1757., 1193.],         ...,         [2005.,  110., 2987.],         [2018., 1602., 1373.],         [2016., 2014., 1498.]]),  ['Low-Mid', 'Low-Mid', 'Low-Mid', 'Low-Mid', 'Low-Mid', ..., 'Cheap', 'Low-Mid', 'Mid-High', 'Mid-High', 'Low-Mid']  Length: 4784  Categories (5, object): ['Cheap' &lt; 'Low-Mid' &lt; 'Mid-High' &lt; 'Expensive' &lt; 'Super-Expensive'])   clf = DecisionTreeClassifier(random_state = 34)# max_depth = 5) clf.fit(X_train, y_train)   DecisionTreeClassifier(random_state=34)   cross_val_score(clf, X_train, y_train , cv=10)   array([0.77244259, 0.76617954, 0.76617954, 0.76200418, 0.80125523,        0.75313808, 0.76569038, 0.78870293, 0.77824268, 0.76987448])   clf.score(X_test, y_test)   0.772765246449457   clf.predict(clean_test_data)   /usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names   f\"X has feature names, but {self.__class__.__name__} was fitted without\"      array(['Cheap', 'Cheap', 'Expensive', ..., 'Cheap', 'Cheap', 'Expensive'],       dtype=object)   ","categories": [],
        "tags": [],
        "url": "/Decision-Trees/",
        "teaser": null
      },{
        "title": "Bert Analysis",
        "excerpt":"!pip install -q tf-models-official !pip install -q tensorflow-text !pip install -q tf-models-official==2.3.0Sa   \u001b[K     |████████████████████████████████| 1.1MB 6.9MB/s  \u001b[K     |████████████████████████████████| 51kB 6.8MB/s  \u001b[K     |████████████████████████████████| 37.6MB 124kB/s  \u001b[K     |████████████████████████████████| 174kB 50.3MB/s  \u001b[K     |████████████████████████████████| 1.2MB 46.8MB/s  \u001b[K     |████████████████████████████████| 276kB 41.9MB/s  \u001b[K     |████████████████████████████████| 102kB 12.0MB/s  \u001b[K     |████████████████████████████████| 358kB 44.5MB/s  \u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone   Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone   Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone \u001b[K     |████████████████████████████████| 3.4MB 5.0MB/s  \u001b[K     |████████████████████████████████| 849kB 5.4MB/s  \u001b[?25h   import requests r = requests.post(\"http://3386c69248d9.ngrok.io/\", data={'foo': 'The Role of Saying No is sometimes seen as a luxury that only those in power can afford . But saying no is not merely a privilege reserved for the successful among us . It is also a strategy that can help you become successful . Steve Jobs famously said, “People think focus means saying yes to the thing you’ve got to focus on. But that’s not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully’re not always saying yes,” Steve Jobs said. “If you don’t guard your time, people will steal it from you,\" says Pedro Sorrentino. ‘If you are not guarding your time.’ says Sorrentinos.‘ If you want to say no to distractions, it means you need to say yes, it is the only productivity hack,’ he says. You may have to try many things to discover what works'}) # And done. print(r.text) # displays the result body.    The Role of Saying No is sometimes seen as a luxury that only those in power can afford . But saying no is not merely a privilege reserved for the successful among us . It is also a strategy that can help you become successful   #Import dependencies #Import necessary dependancies import os  import numpy as np import matplotlib.pyplot as plt  import tensorflow as tf  import tensorflow_hub as hub import tensorflow_datasets as tfds tfds.disable_progress_bar()  from official.modeling import tf_utils from official import nlp from official.nlp import bert  # Load the required submodules import official.nlp.optimization import official.nlp.bert.bert_models import official.nlp.bert.configs import official.nlp.bert.run_classifier import official.nlp.bert.tokenization import official.nlp.data.classifier_data_lib import official.nlp.modeling.losses import official.nlp.modeling.models import official.nlp.modeling.networks  from google.colab import files from google.colab import drive import pandas as pd import io import numpy    gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\" tf.io.gfile.listdir(gs_folder_bert)   ['bert_config.json',  'bert_model.ckpt.data-00000-of-00001',  'bert_model.ckpt.index',  'vocab.txt']   hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"   uploaded = files.upload()  dataset = pd.read_csv(io.BytesIO(uploaded['profitaa.csv']))  df2 = dataset.sample(frac=0.8, random_state=0) df2_test = dataset.drop(df2.index)        Upload widget is only available when the cell has been executed in the  current browser session. Please rerun this cell to enable.     Saving summary.csv to summary.csv   df2.shape  df2[\"Relevancy_Score\"].isnull().values.any()  df3 = df2.dropna()  df3.describe().transpose()                            count       unique       top       freq                       headline       1487       1487       \\nCreate a comfortable home for both hamsters....       1                 title       1487       1487       How to Tell a Middle School Boy You Like Him       1                 text       1487       1486       ,,       2            df3[\"Relevancy_Score\"].isnull().values.any()  df3_test = df2_test.dropna()      # Set up tokenizer to generate Tensorflow dataset tokenizer = bert.tokenization.FullTokenizer(     vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),      do_lower_case=True)s  print(\"Vocab size:\", len(tokenizer.vocab))   Vocab size: 30522   tokens = tokenizer.tokenize(\"Hello TensorFlow!\") print(tokens) ids = tokenizer.convert_tokens_to_ids(tokens) print(ids)   ['hello', 'tensor', '##flow', '!'] [7592, 23435, 12314, 999]   tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])   [101, 102]   def encode_sentencer(s):    tokens = list(tokenizer.tokenize(s))    tokens.append('[SEP]')    return tokenizer.convert_tokens_to_ids(tokens)  sentence1 = tf.ragged.constant([     encode_sentencer(s) for s in df3[\"Sentence_1\"]]) sentence2 = tf.ragged.constant([     encode_sentencer(s) for s in df3[\"Sentence_2\"]])   print(\"Sentence1 shape:\", sentence1.shape.as_list()) print(\"Sentence2 shape:\", sentence2.shape.as_list()) print(sentence1[0])   Sentence1 shape: [1487, None] Sentence2 shape: [1487, None] tf.Tensor( [ 4638  2065  1996 10654  6238  1005  1055  4373  2203  2003  4954  1012   1010  3198  1996  9004  4497  1013  8843  2121  2065  2017  2064  5047   1996 10654 15608  1012  1010  2298  2005 10654 15608  2008  2031 12538  15695  1010  4408  2159  1010  1998  2024  3227  5379  1010  2065  1996  10654  6238  2003  1037  2978  5376  2100  2012  2034  2043  2017  5047   2032  1013  2014  1010  2123  1005  1056  4737  2008  2003  2025  1037   2919  2518  1012  1010  5454  1037 10654  6238  2008  2017  2066  1012    102], shape=(85,), dtype=int32)   cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0] input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1) _ = plt.pcolormesh(input_word_ids.to_tensor())      input_mask = tf.ones_like(input_word_ids).to_tensor()  plt.pcolormesh(input_mask)   &lt;matplotlib.collections.QuadMesh at 0x7faa562b40f0&gt;      type_cls = tf.zeros_like(cls) type_s1 = tf.zeros_like(sentence1) type_s2 = tf.ones_like(sentence2) input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()  plt.pcolormesh(input_type_ids)   &lt;matplotlib.collections.QuadMesh at 0x7faa56299320&gt;      def encode_sentence(s, tokenizer):    tokens = list(tokenizer.tokenize(s))    tokens.append('[SEP]')    return tokenizer.convert_tokens_to_ids(tokens)   def bert_encode(glue_dict, tokenizer):   num_examples = len(glue_dict[\"Sentence_1\"])      sentence1 = tf.ragged.constant([       encode_sentence(s, tokenizer)       for s in np.array(glue_dict[\"Sentence_1\"])])   sentence2 = tf.ragged.constant([       encode_sentence(s, tokenizer)        for s in np.array(glue_dict[\"Sentence_2\"])])    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]   input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)    input_mask = tf.ones_like(input_word_ids).to_tensor()    type_cls = tf.zeros_like(cls)   type_s1 = tf.zeros_like(sentence1)   type_s2 = tf.ones_like(sentence2)   input_type_ids = tf.concat(       [type_cls, type_s1, type_s2], axis=-1).to_tensor()    inputs = {       'input_word_ids': input_word_ids.to_tensor(),       'input_mask': input_mask,       'input_type_ids': input_type_ids}    return inputs   df_train = bert_encode(df3, tokenizer) #df_labels = df3['Relevancy_Score'].div(5)  df_test = bert_encode(df3_test, tokenizer) df_test_labels = df3_test['Relevancy_Score'].div(5)  print(df_train) print(df_test)   ---------------------------------------------------------------------------  KeyError                                  Traceback (most recent call last)  /usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)    2897             try: -&gt; 2898                 return self._engine.get_loc(casted_key)    2899             except KeyError as err:   pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()   pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()   pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()   pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()   KeyError: 'Sentence_1'   The above exception was the direct cause of the following exception:   KeyError                                  Traceback (most recent call last)  &lt;ipython-input-29-1e8639691852&gt; in &lt;module&gt;() ----&gt; 1 df_train = bert_encode(df3, tokenizer)       2 #df_labels = df3['Relevancy_Score'].div(5)       3        4 df_test = bert_encode(df3_test, tokenizer)       5 df_test_labels = df3_test['Relevancy_Score'].div(5)   &lt;ipython-input-27-02cbe3d43c41&gt; in bert_encode(glue_dict, tokenizer)       6        7 def bert_encode(glue_dict, tokenizer): ----&gt; 8   num_examples = len(glue_dict[\"Sentence_1\"])       9       10   sentence1 = tf.ragged.constant([   /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in __getitem__(self, key)    2904             if self.columns.nlevels &gt; 1:    2905                 return self._getitem_multilevel(key) -&gt; 2906             indexer = self.columns.get_loc(key)    2907             if is_integer(indexer):    2908                 indexer = [indexer]   /usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)    2898                 return self._engine.get_loc(casted_key)    2899             except KeyError as err: -&gt; 2900                 raise KeyError(key) from err    2901     2902         if tolerance is not None:   KeyError: 'Sentence_1'   for key, value in df_train.items():   print(f'{key:15s} shape: {value.shape}')  print(f'df_labels shape: {df_labels.shape}')   input_word_ids  shape: (2396, 125) input_mask      shape: (2396, 125) input_type_ids  shape: (2396, 125) df_labels shape: (2396,)   import json  bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\") config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())  bert_config = bert.configs.BertConfig.from_dict(config_dict)  config_dict   {'attention_probs_dropout_prob': 0.1,  'hidden_act': 'gelu',  'hidden_dropout_prob': 0.1,  'hidden_size': 768,  'initializer_range': 0.02,  'intermediate_size': 3072,  'max_position_embeddings': 512,  'num_attention_heads': 12,  'num_hidden_layers': 12,  'type_vocab_size': 2,  'vocab_size': 30522}   print(bert.bert_models) bert_classifier, bert_encoder = bert.bert_models.classifier_model(    bert_config, num_labels=1)   &lt;module 'official.nlp.bert.bert_models' from '/usr/local/lib/python3.6/dist-packages/official/nlp/bert/bert_models.py'&gt;   tf.keras.utils.plot_model(bert_classifier, show_shapes=True, dpi=48)   glue_batch = {key: val[:10] for key, val in df_train.items()}  bert_classifier(     glue_batch, training=True ).numpy()   array([[-0.17102675],        [ 0.07586445],        [ 0.01797828],        [-0.19046766],        [-0.06210539],        [-0.09033417],        [ 0.01831295],        [-0.08006046],        [-0.22937882],        [-0.03147416]], dtype=float32)   tf.keras.utils.plot_model(bert_encoder, show_shapes=True, dpi=48)   [print(i.shape, i.dtype) for i in bert_classifier.inputs] [print(o.shape, o.dtype) for o in bert_classifier.outputs] [print(l.name, l.input_shape, l.dtype) for l in bert_classifier.layers] bert_classifier.summary()   (None, None) &lt;dtype: 'int32'&gt; (None, None) &lt;dtype: 'int32'&gt; (None, None) &lt;dtype: 'int32'&gt; (None, 1) &lt;dtype: 'float32'&gt; input_word_ids [(None, None)] int32 input_mask [(None, None)] int32 input_type_ids [(None, None)] int32 transformer_encoder [(None, None), (None, None), (None, None)] float32 dropout_1 (None, 768) float32 classification (None, 768) float32 Model: \"bert_classifier\" __________________________________________________________________________________________________ Layer (type)                    Output Shape         Param #     Connected to                      ================================================================================================== input_word_ids (InputLayer)     [(None, None)]       0                                             __________________________________________________________________________________________________ input_mask (InputLayer)         [(None, None)]       0                                             __________________________________________________________________________________________________ input_type_ids (InputLayer)     [(None, None)]       0                                             __________________________________________________________________________________________________ transformer_encoder (Transforme [(None, None, 768),  109482240   input_word_ids[0][0]                                                                               input_mask[0][0]                                                                                   input_type_ids[0][0]              __________________________________________________________________________________________________ dropout_1 (Dropout)             (None, 768)          0           transformer_encoder[0][1]         __________________________________________________________________________________________________ classification (Classification) (None, 1)            769         dropout_1[0][0]                   ================================================================================================== Total params: 109,483,009 Trainable params: 109,483,009 Non-trainable params: 0 __________________________________________________________________________________________________   checkpoint = tf.train.Checkpoint(model=bert_encoder) checkpoint.restore(     os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()   &lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe309295a20&gt;   # Set up epochs and steps epochs = 2 batch_size = 32 eval_batch_size = 32  train_data_size = len(df_labels) steps_per_epoch = int(train_data_size / batch_size) num_train_steps = steps_per_epoch * epochs warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)  # creates an optimizer with learning rate schedule optimizer = nlp.optimization.create_optimizer(     2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)   type(optimizer)   official.nlp.optimization.AdamWeightDecay   metric = [tf.keras.metrics.Accuracy('accuracy', dtype=tf.float32)] loss = tf.keras.losses.MeanAbsoluteError()  bert_classifier.compile(     optimizer=optimizer,     loss=loss,     metrics=['mse', 'mae', 'mape', 'cosine_similarity','accuracy'])  bert_classifier.fit(       df_train, df_labels,       batch_size=32,       epochs=epochs)   Epoch 1/2 75/75 [==============================] - 3287s 44s/step - loss: 0.3301 - mse: 0.1647 - mae: 0.3301 - mape: 26140062.6908 - cosine_similarity: 0.8531 - accuracy: 0.0810 Epoch 2/2 75/75 [==============================] - 3245s 43s/step - loss: 0.1562 - mse: 0.0406 - mae: 0.1562 - mape: 11219197.4079 - cosine_similarity: 0.8746 - accuracy: 0.1360      &lt;tensorflow.python.keras.callbacks.History at 0x7fe3034ba908&gt;   export_dir='./saved_model2' tf.saved_model.save(bert_classifier, export_dir=export_dir)    WARNING:absl:Found untraced functions such as self_attention_layer_call_fn, self_attention_layer_call_and_return_conditional_losses, attention_output_layer_call_fn, attention_output_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn while saving (showing 5 of 840). These functions will not be directly callable after loading. WARNING:absl:Found untraced functions such as self_attention_layer_call_fn, self_attention_layer_call_and_return_conditional_losses, attention_output_layer_call_fn, attention_output_layer_call_and_return_conditional_losses, dropout_1_layer_call_fn while saving (showing 5 of 840). These functions will not be directly callable after loading.   INFO:tensorflow:Assets written to: ./saved_model2/assets   INFO:tensorflow:Assets written to: ./saved_model2/assets      bert_classifier.evaluate(df_test,df_test_labels)   19/19 [==============================] - 172s 9s/step - loss: 0.1405 - mse: 0.0355 - mae: 0.1405 - mape: 12436080.0000 - cosine_similarity: 0.8980 - accuracy: 0.1405      [0.140456423163414,  0.03549607843160629,  0.140456423163414,  12436080.0,  0.8979933261871338,  0.14046822488307953]   my_examples = bert_encode(     glue_dict = {         'Sentence_1':[             'The rain in Spain falls mainly on the plain.',             'Look I fine tuned BERT.',             'I am alive.'],         'Sentence_2':[             'It mostly rains on the flat lands of Spain.',             'Is it working? This does not match.',             \"I am alive.\"]     },     tokenizer=tokenizer) result = bert_classifier(my_examples, training=False)  print(result)  #result = tf.argmax(result).numpy()  array = result.numpy()  def normalize(value): \tnormalized = (value + 1) / (2); \treturn normalized;   for i in array:   x = normalize(i)   print()   tf.Tensor( [[0.6340052]  [0.1166743]  [0.7393124]], shape=(3, 1), dtype=float32)   !pip install h5py   Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0) Requirement already satisfied: numpy&gt;=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.19.5)   bert_classifier.save(\"./model.h5\") print(\"Saved model to disk\")   Saved model to disk   from google.colab import drive drive.mount('/content/drive')   Mounted at /content/drive   reloaded = tf.saved_model.load(export_dir) reloaded_result = reloaded([my_examples['input_word_ids'],                             my_examples['input_mask'],                             my_examples['input_type_ids']], training=False)  original_result = bert_classifier(my_examples, training=False)  # The results are (nearly) identical: print(original_result.numpy()) print() print(reloaded_result.numpy())   [[0.6340052]  [0.1166743]  [0.7393124]]  [[0.63400537]  [0.11667421]  [0.7393121 ]]      uploaded2 = files.upload()   predict = pd.read_csv(io.BytesIO(uploaded2['valtest.csv']))  predicter = bert_encode(glue_dict={\"Sentence_1\":predict['Sentence_1'],'Sentence_2': predict[\"Sentence_2\"]}, tokenizer = tokenizer)  solutions = bert_classifier.predict(predicter)       Upload widget is only available when the cell has been executed in the  current browser session. Please rerun this cell to enable.     Saving valtest.csv to valtest.csv   #a = (df.abs())  #b = normalized_predict  diff_pred = (solutions.tolist())  #print(diff_pred)  new_solutions = [] for i in diff_pred:   for x in i:     new_solutions.append(x)  ans = pd.Series(new_solutions)-predict['Relevancy_Score'].div(5)  print(ans.abs().describe().transpose())     count    750.000000 mean       0.126923 std        0.096725 min        0.000204 25%        0.052139 50%        0.108334 75%        0.178874 max        0.557846 dtype: float64    bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'   map_name_to_handle = {     'bert_en_uncased_L-12_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',     'bert_en_cased_L-12_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',     'bert_multi_cased_L-12_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',     'small_bert/bert_en_uncased_L-2_H-128_A-2':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',     'small_bert/bert_en_uncased_L-2_H-256_A-4':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',     'small_bert/bert_en_uncased_L-2_H-512_A-8':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',     'small_bert/bert_en_uncased_L-2_H-768_A-12':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',     'small_bert/bert_en_uncased_L-4_H-128_A-2':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',     'small_bert/bert_en_uncased_L-4_H-256_A-4':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',     'small_bert/bert_en_uncased_L-4_H-512_A-8':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',     'small_bert/bert_en_uncased_L-4_H-768_A-12':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',     'small_bert/bert_en_uncased_L-6_H-128_A-2':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',     'small_bert/bert_en_uncased_L-6_H-256_A-4':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',     'small_bert/bert_en_uncased_L-6_H-512_A-8':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',     'small_bert/bert_en_uncased_L-6_H-768_A-12':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',     'small_bert/bert_en_uncased_L-8_H-128_A-2':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',     'small_bert/bert_en_uncased_L-8_H-256_A-4':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',     'small_bert/bert_en_uncased_L-8_H-512_A-8':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',     'small_bert/bert_en_uncased_L-8_H-768_A-12':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',     'small_bert/bert_en_uncased_L-10_H-128_A-2':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',     'small_bert/bert_en_uncased_L-10_H-256_A-4':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',     'small_bert/bert_en_uncased_L-10_H-512_A-8':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',     'small_bert/bert_en_uncased_L-10_H-768_A-12':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',     'small_bert/bert_en_uncased_L-12_H-128_A-2':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',     'small_bert/bert_en_uncased_L-12_H-256_A-4':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',     'small_bert/bert_en_uncased_L-12_H-512_A-8':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',     'small_bert/bert_en_uncased_L-12_H-768_A-12':         'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',     'albert_en_base':         'https://tfhub.dev/tensorflow/albert_en_base/2',     'electra_small':         'https://tfhub.dev/google/electra_small/2',     'electra_base':         'https://tfhub.dev/google/electra_base/2',     'experts_pubmed':         'https://tfhub.dev/google/experts/bert/pubmed/2',     'experts_wiki_books':         'https://tfhub.dev/google/experts/bert/wiki_books/2',     'talking-heads_base':         'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1', }  map_model_to_preprocess = {     'bert_en_uncased_L-12_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'bert_en_cased_L-12_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/2',     'small_bert/bert_en_uncased_L-2_H-128_A-2':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-2_H-256_A-4':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-2_H-512_A-8':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-2_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-4_H-128_A-2':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-4_H-256_A-4':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-4_H-512_A-8':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-4_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-6_H-128_A-2':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-6_H-256_A-4':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-6_H-512_A-8':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-6_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-8_H-128_A-2':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-8_H-256_A-4':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-8_H-512_A-8':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-8_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-10_H-128_A-2':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-10_H-256_A-4':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-10_H-512_A-8':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-10_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-12_H-128_A-2':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-12_H-256_A-4':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-12_H-512_A-8':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'small_bert/bert_en_uncased_L-12_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'bert_multi_cased_L-12_H-768_A-12':         'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/2',     'albert_en_base':         'https://tfhub.dev/tensorflow/albert_en_preprocess/2',     'electra_small':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'electra_base':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'experts_pubmed':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'experts_wiki_books':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2',     'talking-heads_base':         'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2', }  tfhub_handle_encoder = map_name_to_handle[bert_model_name] tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]  print(f'BERT model selected           : {tfhub_handle_encoder}') print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')     BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1 Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2   #Using the PREPROCESSING Model bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) text_test = ['this is such an amazing movie!', 'I am Alive. I am Indebted','Praise to God almighty'] text_preprocessed = bert_preprocess_model(df2.Sentence_1)  print(text_preprocessed)  print(f'Keys       : {list(text_preprocessed.keys())}') print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}') print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}') print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}') print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')  #Using the BERT Model bert_model = hub.KerasLayer(tfhub_handle_encoder) bert_results = bert_model(text_preprocessed)  print(bert_results) print(f'Loaded BERT: {tfhub_handle_encoder}') print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}') print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}') print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}') print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')   ---------------------------------------------------------------------------  ValueError                                Traceback (most recent call last)  &lt;ipython-input-6-d818c6d3088a&gt; in &lt;module&gt;()       2 bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)       3 text_test = ['this is such an amazing movie!', 'I am Alive. I am Indebted','Praise to God almighty'] ----&gt; 4 text_preprocessed = bert_preprocess_model(df2.Sentence_1)       5        6 print(text_preprocessed)   /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)    1010         with autocast_variable.enable_auto_cast_variables(    1011             self._compute_dtype_object): -&gt; 1012           outputs = call_fn(inputs, *args, **kwargs)    1013     1014         if self._activity_regularizer:   /usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py in call(self, inputs, training)     235       result = smart_cond.smart_cond(training,     236                                      lambda: f(training=True), --&gt; 237                                      lambda: f(training=False))     238      239     # Unwrap dicts returned by signatures.   /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)      54       return true_fn()      55     else: ---&gt; 56       return false_fn()      57   else:      58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,   /usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py in &lt;lambda&gt;()     235       result = smart_cond.smart_cond(training,     236                                      lambda: f(training=True), --&gt; 237                                      lambda: f(training=False))     238      239     # Unwrap dicts returned by signatures.   /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py in _call_attribute(instance, *args, **kwargs)     666      667 def _call_attribute(instance, *args, **kwargs): --&gt; 668   return instance.__call__(*args, **kwargs)     669      670    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)     826     tracing_count = self.experimental_get_tracing_count()     827     with trace.Trace(self._name) as tm: --&gt; 828       result = self._call(*args, **kwds)     829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"     830       new_tracing_count = self.experimental_get_tracing_count()   /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)     869       # This is the first call of __call__, so we have to initialize.     870       initializers = [] --&gt; 871       self._initialize(args, kwds, add_initializers_to=initializers)     872     finally:     873       # At this point we know that the initialization is complete (or less   /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)     724     self._concrete_stateful_fn = (     725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access --&gt; 726             *args, **kwds))     727      728     def invalid_creator_scope(*unused_args, **unused_kwds):   /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)    2967       args, kwargs = None, None    2968     with self._lock: -&gt; 2969       graph_function, _ = self._maybe_define_function(args, kwargs)    2970     return graph_function    2971    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)    3312     if self.input_signature is None or args is not None or kwargs is not None:    3313       args, kwargs, flat_args, filtered_flat_args = \\ -&gt; 3314           self._function_spec.canonicalize_function_inputs(*args, **kwargs)    3315     else:    3316       flat_args, filtered_flat_args = [None], []   /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in canonicalize_function_inputs(self, *args, **kwargs)    2695     2696     if self._input_signature is None: -&gt; 2697       inputs, flat_inputs, filtered_flat_inputs = _convert_numpy_inputs(inputs)    2698       kwargs, flat_kwargs, filtered_flat_kwargs = _convert_numpy_inputs(kwargs)    2699       return (inputs, kwargs, flat_inputs + flat_kwargs,   /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _convert_numpy_inputs(inputs)    2755         raise TypeError(\"The output of __array__ must be an np.ndarray \"    2756                         \"(got {} from {}).\".format(type(a), type(value))) -&gt; 2757       flat_inputs[index] = constant_op.constant(a)    2758       filtered_flat_inputs.append(flat_inputs[index])    2759       need_packing = True   /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)     263   \"\"\"     264   return _constant_impl(value, dtype, shape, name, verify_shape=False, --&gt; 265                         allow_broadcast=True)     266      267    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)     274       with trace.Trace(\"tf.constant\"):     275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape) --&gt; 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)     277      278   g = ops.get_default_graph()   /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)     299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):     300   \"\"\"Implementation of eager constant.\"\"\" --&gt; 301   t = convert_to_eager_tensor(value, ctx, dtype)     302   if shape is None:     303     return t   /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)      96       dtype = dtypes.as_dtype(dtype).as_datatype_enum      97   ctx.ensure_initialized() ---&gt; 98   return ops.EagerTensor(value, ctx.device_name, dtype)      99      100    ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).   sentence1_input = tf.keras.Input(shape=(), dtype=tf.string, name = \"Sentence1\") sentence2_input = tf.keras.Input(shape=(), dtype=tf.string, name = \"Sentence2\") relevancyinput = tf.keras.Input(shape=(1), name = \"RelevancyScore\")  sentencepreprocessing = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing1')  sentence1_preprocessing = sentencepreprocessing(sentence1_input) sentence2_preprocessing = sentencepreprocessing(sentence2_input)  sentencebert = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='BERT_encoder1')  sentence1_bert = sentencebert(sentence1_preprocessing) sentence2_bert = sentencebert(sentence2_preprocessing)   concatenate_all = tf.keras.layers.concatenate([relevancyinput, sentence1_bert[\"pooled_output\"], sentence2_bert[\"pooled_output\"]]) neta2 = tf.keras.layers.Dense(1024, activation=None, name='sts1')(concatenate_all) neta3 = tf.keras.layers.Dense(768, activation=None, name='sts2')(neta2) net = tf.keras.layers.Dense(1, activation=None, name='sts')(neta3)  model = tf.keras.Model(     inputs=[sentence1_input, sentence2_input],     outputs=[net], )   def CosineSimilarity(a,b):   #a**b/squareroot(summation of a^2)**squaeroot(summation of b^2)   for i in a:     for o im b:       a *   [print(i.shape, i.dtype) for i in model.inputs] [print(o.shape, o.dtype) for o in model.outputs] [print(l.name, l.input_shape, l.dtype) for l in model.layers] model.summary()   (None,) &lt;dtype: 'string'&gt; (None,) &lt;dtype: 'string'&gt; (None, 1) &lt;dtype: 'float32'&gt; Sentence1 [(None,)] string Sentence2 [(None,)] string preprocessing1 None float32 BERT_encoder1 {'input_word_ids': (None, 128), 'input_mask': (None, 128), 'input_type_ids': (None, 128)} float32 concatenate_5 [(None, 512), (None, 512)] float32 sts1 (None, 1024) float32 sts2 (None, 1024) float32 sts (None, 768) float32 Model: \"model_5\" __________________________________________________________________________________________________ Layer (type)                    Output Shape         Param #     Connected to                      ================================================================================================== Sentence1 (InputLayer)          [(None,)]            0                                             __________________________________________________________________________________________________ Sentence2 (InputLayer)          [(None,)]            0                                             __________________________________________________________________________________________________ preprocessing1 (KerasLayer)     {'input_word_ids': ( 0           Sentence1[0][0]                                                                                    Sentence2[0][0]                   __________________________________________________________________________________________________ BERT_encoder1 (KerasLayer)      {'pooled_output': (N 28763649    preprocessing1[0][0]                                                                               preprocessing1[0][1]                                                                               preprocessing1[0][2]                                                                               preprocessing1[1][0]                                                                               preprocessing1[1][1]                                                                               preprocessing1[1][2]              __________________________________________________________________________________________________ concatenate_5 (Concatenate)     (None, 1024)         0           BERT_encoder1[0][5]                                                                                BERT_encoder1[1][5]               __________________________________________________________________________________________________ sts1 (Dense)                    (None, 1024)         1049600     concatenate_5[0][0]               __________________________________________________________________________________________________ sts2 (Dense)                    (None, 768)          787200      sts1[0][0]                        __________________________________________________________________________________________________ sts (Dense)                     (None, 1)            769         sts2[0][0]                        ================================================================================================== Total params: 30,601,218 Trainable params: 1,837,569 Non-trainable params: 28,763,649 __________________________________________________________________________________________________                                                                                                                                                                                                                         #Defining the loss function loss = tf.keras.losses.MeanAbsoluteError() metrics = tf.metrics.Accuracy()  #Defining the Optimizer epochs = 4 steps_per_epoch = 5 num_train_steps = steps_per_epoch * epochs num_warmup_steps = int(0.1*num_train_steps)  init_lr = 3e-5 optimizer = optimization.create_optimizer(init_lr=init_lr,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')  #Loading the BERT Model and Training model.compile(optimizer=optimizer,loss=loss,metrics=['mse', 'mae', 'mape', 'cosine_similarity','accuracy'])  print(df3[\"Relevancy_Score\"])  sentence1_data = (df3.Sentence_1).astype(\"string\") relevancydata = pd.to_numeric((df3['Relevancy_Score']),errors='coerce') normalized = relevancydata.div(5) sentence2_data = (df3.Sentence_2).astype(\"string\") print(normalized) print(type(sentence1_data[1]))  model.fit(     {\"Sentence1\": sentence1_data, \"Sentence2\": sentence2_data}, y = normalized, epochs=epochs )  print(f'Training model with {tfhub_handle_encoder}')   776     4.0 1424    3.0 227     4.7 2402    0.8 104     5.0        ...  409     4.4 1623    2.8 115     5.0 288     4.6 2510    0.4 Name: Relevancy_Score, Length: 2305, dtype: float64 776     0.80 1424    0.60 227     0.94 2402    0.16 104     1.00         ...  409     0.88 1623    0.56 115     1.00 288     0.92 2510    0.08 Name: Relevancy_Score, Length: 2305, dtype: float64 &lt;class 'str'&gt; Epoch 1/4 73/73 [==============================] - 312s 4s/step - loss: 0.4590 - mse: 0.3090 - mae: 0.4590 - mape: 98865381.2973 - cosine_similarity: 0.9011 - accuracy: 0.0438 Epoch 2/4 73/73 [==============================] - 306s 4s/step - loss: 0.4504 - mse: 0.2993 - mae: 0.4504 - mape: 89167020.2162 - cosine_similarity: 0.9108 - accuracy: 0.0476 Epoch 3/4 73/73 [==============================] - 308s 4s/step - loss: 0.4469 - mse: 0.2979 - mae: 0.4469 - mape: 92691536.1622 - cosine_similarity: 0.9073 - accuracy: 0.0570 Epoch 4/4 73/73 [==============================] - 305s 4s/step - loss: 0.4534 - mse: 0.3041 - mae: 0.4534 - mape: 95444199.7838 - cosine_similarity: 0.9046 - accuracy: 0.0450 Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1      df3_test = df2_test.dropna()  print(df3_test.describe().transpose())  sentence1_test = df3_test[\"Sentence_1\"] sentence2_test = df3_test[\"Sentence_2\"] relevancy_test = df3_test[\"Relevancy_Score\"] normalized_test = relevancy_test.div(5) metric = tf.metrics.CosineSimilarity() losses = model.evaluate({\"Sentence1\": sentence1_test, \"Sentence2\": sentence2_test}, {\"sts\": normalized_test})  print(f'Loss: {losses}')    ---------------------------------------------------------------------------  NameError                                 Traceback (most recent call last)  &lt;ipython-input-1-9144b8188c9a&gt; in &lt;module&gt;()       2        3  ----&gt; 4 df3_test = df2_test.dropna()       5        6 print(df3_test.describe().transpose())   NameError: name 'df2_test' is not defined   uploaded2 = files.upload()  predict = pd.read_csv(io.BytesIO(uploaded2['predict.csv']))   sentence1_predict = predict[\"Sentence_1\"] sentence2_predict = predict[\"Sentence_2\"] relevancy_predict = predict[\"Relevancy_Score\"] normalized_predict = relevancy_predict.div(5)   solutions = model.predict({\"Sentence1\": sentence1_predict, \"Sentence2\": sentence2_predict})       Upload widget is only available when the cell has been executed in the  current browser session. Please rerun this cell to enable.     Saving predict.csv to predict (1).csv       newList = [] for x in solutions:     newList.append(x*5)  df = pd.DataFrame(solutions) fileName = 'prdict.csv' df.to_csv(fileName)  print(df)              0 0  -0.296717 1  -0.083558 2   0.316203 3  -0.035383 4   0.340398 5   0.672012 6   0.557940 7   0.458394 8   0.027319 9   0.368085 10  0.478323 11  0.515013 12  1.077829 13  0.733209 14  0.935209 15  0.316129 16  0.044420 17  0.027349 18  0.295618 19  0.381707 20  0.388599 21  0.310701 22  0.688457 23 -0.062880 24  0.946695 25 -0.148819 26  0.647570 27  0.811228 28  0.595808 29  0.022950   a = (df.abs())  b = normalized_predict  diff_pred = (b).subtract(a)  print(diff_pred[0].abs())  diff_pred[0].abs().describe().transpose()  print((1 - diff_pred[0].abs()).sum())   ---------------------------------------------------------------------------  NameError                                 Traceback (most recent call last)  &lt;ipython-input-1-a4cb93cb6cf5&gt; in &lt;module&gt;() ----&gt; 1 a = (df.abs())       2        3 b = normalized_predict       4        5 diff_pred = (b).subtract(a)   NameError: name 'df' is not defined   dataset_name = 'imdb' saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_')) model.save(saved_model_path, include_optimizer=False)   WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading. WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 310). These functions will not be directly callable after loading.   ","categories": [],
        "tags": [],
        "url": "/BERT-Analysis/",
        "teaser": null
      }]
